{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcfd94a",
   "metadata": {},
   "source": [
    "# Segmentation · Extraction · Behavior — session flow & data layout\n",
    "\n",
    "This notebook processes one **session** at a time (e.g., a single date/animal/epoch). It aligns behavior and imaging on an **absolute wall-time axis**, extracts ROI fluorescence from motion-corrected movies, and saves a consolidated HDF5 suitable for downstream analysis. It also includes a resumable per-trial cache and an interactive ROI explorer.\n",
    "\n",
    "---\n",
    "\n",
    "## Session directory layout\n",
    "\n",
    "~~~text\n",
    "/Volumes/MossLab/ImagingData/<YYYYMMDD>/<AnimalID>/<Epoch>/\n",
    "├── RAW ScanImage TIFFs                    # one stack per trial; names carry a trailing index, e.g., ..._00037.tif\n",
    "├── Motion-corrected TIFFs (MCOR)          # same stems with \"_mcor\" suffix, e.g., ..._00037_mcor.tif\n",
    "├── *_Events.csv                           # behavior event logs; one or more files for the session\n",
    "├── <session>.h5                           # DAQ/TTL file (5 kHz ImagingWindow TTL used for absolute timing)\n",
    "├── trace_cache/                           # created by this notebook; per-trial extraction cache\n",
    "│   ├── <stem>_mcor.npz                    # arrays: F, dFF, Z\n",
    "│   └── <stem>_mcor.json                   # per-trial metadata\n",
    "└── <export>.hdf5                            # consolidated output written by this notebook (path = export_hdf5)\n",
    "~~~\n",
    "\n",
    "\n",
    "> Notes  \n",
    "> • MCOR files can live in the session root (as above). If you store them in a subfolder, the `resolved_trial_to_mcor` mapping will still resolve as long as stems match.  \n",
    "> • Behavior CSVs may be multiple files; the notebook concatenates them into one continuous timeline.\n",
    "\n",
    "---\n",
    "\n",
    "## High-level flow\n",
    "\n",
    "1. **Configure session and build utility functions**: Set `parent_dir`, sampling rate `fr`, behavior column names (`behavior_time_col`, `behavior_event_col`), odor duration (`odor_duration_s`), and match tolerance (`TOL_SECONDS`).\n",
    "\n",
    "2. **Build pseudo trial video for ROI extraction**: Extract random frames from subset of trial videos during the 1s odor presentation and 1s after odor offset. Compile these into a new tiff stack for feeding into CaImAn CNMF ROI segmentation\n",
    "\n",
    "3. **Run CaImAn CNMF**: Run CNMF on the pseudotrial video using parameters definied in step 1. Save a resumable copy of variables (pickle) at the end.\n",
    "\n",
    "4. **Clean ROIs based on morphological parameters**: Filter out large, small, and convoluted perimeter glomeruli with parameters defined in step 1.\n",
    "    \n",
    "5. **Read absolute trial timing from the H5**: Parse the 5 kHz **ImagingWindow TTL** to get trial onsets, convert them to **absolute POSIX seconds** (wall time). These become the corrected, ground-truth imaging onsets used downstream. THen, from each RAW TIFF’s metadata, collect its **first-frame POSIX timestamp** and the file **stem**. These are used to pair H5 onsets with real image stacks. To do this, we match each H5 onset to the nearest RAW TIFF first-frame within **± `TOL_SECONDS`**. Store the aligned pairs (stems + times). This establishes the imaging timeline.\n",
    "\n",
    "6. **Behavior ingestion & trial parse**  \n",
    "   - Find all `*_Events.csv`, **order** them deterministically (by your chosen rule; e.g., first number in filename), and **concatenate** into a continuous behavior timeline.  \n",
    "   - Define trial windows and **outcomes** using the window from the first **`Response`** to the subsequent **`Trial Interval`**:  \n",
    "     - Any **`Reward L/R`** in the window → **Hit**  \n",
    "     - Else if **Lick L/R count ≥ 3** → **False Choice**  \n",
    "     - Else → **Miss**\n",
    "\n",
    "We then build `resolved_trial_to_mcor` by matching trials **by trailing number** in filenames (e.g., `_00037`). Unmapped or missing files are reported and skipped. This includes *Resumable per-trial extraction*\n",
    "   For each trial in order:\n",
    "   - If `trace_cache/<stem>_mcor.npz` exists → **load** cached `F`, `dFF`, `Z`.  \n",
    "   - Else → **load** the MCOR movie, extract traces using the cleaned, session-level ROIs, then **save** to `trace_cache/` (NPZ + JSON).  \n",
    "   This makes long runs restart-safe and avoids reprocessing.\n",
    "\n",
    "7. **Consolidated HDF5 export** (`export_hdf5`)  \n",
    "   Write:\n",
    "   - **`/trials/<stem>/`**: per-trial datasets (`F`, `dFF`, `Z`).  \n",
    "   - **`/AlignedBehaviorEvents/`**:  \n",
    "     - `OdorPresentations` using **H5-corrected onsets** (session-relative seconds)  \n",
    "     - `Licks`, `Rewards` retimestamped onto the same corrected timeline (and `*_posix` absolutes)  \n",
    "     - `TrialSummary` table (includes `odor_id`, outcome, corrected start times)  \n",
    "   - **`/AbsoluteAlignment/`**: the onset vector used (pre/post offset), RAW first-frame times + stems, mapping kind (`trial_to_mcor`), and alignment parameters (e.g., `match_tolerance_s`).\n",
    "\n",
    "8. **Interactive ROI Explorer**  \n",
    "   For a selected ROI: show its **footprint** and a **column of subplots (one per odor)**. Individual trial traces are **transparent**, overlaid with **mean traces by outcome** (Hit / False Choice / Miss). Axes share a common y-range for easy comparison, and each subplot lists outcome counts.\n",
    "\n",
    "---\n",
    "\n",
    "## Key variables\n",
    "\n",
    "- `parent_dir` — the session folder (see layout above)  \n",
    "- `fr` — imaging frame rate (Hz), should be definied from raw tiff metadata\n",
    "- `odor_duration_s` — seconds, hard coded, but *in the future* can be extracted from session .h5 file  \n",
    "- `behavior_time_col`, `behavior_event_col` — column names in `*_Events.csv`  \n",
    "- `TOL_SECONDS` — allowed H5↔RAW alignment tolerance *after applying global offset*  \n",
    "- `export_hdf5` — full path to the consolidated output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306e43b",
   "metadata": {},
   "source": [
    "## 1. Environment & Dependencies\n",
    "- Scientific Python stack: `numpy`, `pandas`, `matplotlib`, `tifffile`, `h5py`, `ipywidgets`, `tqdm`\n",
    "- **CaImAn** for CNMF (segmentation + cleaning)\n",
    "- Filesystem times are handled as **POSIX seconds (UTC)**. On macOS we use `st_birthtime`; on Linux we fall back to `st_mtime`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings, json, logging, h5py, re, gc, tempfile, math\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmm\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# CaImAn\n",
    "import caiman as cm\n",
    "from caiman.source_extraction.cnmf import params as caiman_params\n",
    "from caiman.source_extraction import cnmf as cnmf_mod\n",
    "from caiman.cluster import setup_cluster, stop_server\n",
    "from skimage.measure import regionprops\n",
    "import scipy.ndimage\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "warnings.filterwarnings(\"ignore\", message=\".*nperseg.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pkg_resources is deprecated.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8b971",
   "metadata": {},
   "source": [
    "## Configure Paths & Core Parameters\n",
    "\n",
    "**Edit `parent_dir` only.** All other paths derive from it.\n",
    "\n",
    "Other parameters outside of file paths can be adjusted as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896ade7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- YOU MUST EDIT THIS ---\n",
    "parent_dir = Path(\"/Volumes/MossLab/ImagingData/20250905/M169/e1/\")\n",
    "# --- END EDIT ---\n",
    "\n",
    "# Derived paths\n",
    "session_dir = parent_dir / \"processed\"\n",
    "raw_dir = parent_dir / \"raw\"\n",
    "out_dir = session_dir / \"outputs\"\n",
    "mcor_dir = session_dir / \"mcor\"\n",
    "for p in [out_dir]:\n",
    "    p.mkdir(exist_ok=True, parents=True)\n",
    "assert parent_dir.is_dir(), f\"Parent directory does not exist: {parent_dir}\"\n",
    "assert mcor_dir.is_dir(), f\"Input 'mcor' directory not found at: {mcor_dir}\"\n",
    "\n",
    "# Behavior parsing\n",
    "behavior_time_col = 'TimeStamp'   # relative per file; code concatenates\n",
    "behavior_event_col = 'Events'     # e.g., 'Output 1', 'Lick', 'Reward', 'Odor I 3'\n",
    "odor_duration_s = 1.0\n",
    "\n",
    "# Imaging / CNMF defaults\n",
    "num_random_frames_per_trial = 5\n",
    "fr_manual = 24.0\n",
    "decay_time = 0.1\n",
    "dxy = (2, 2)\n",
    "w10 = 20\n",
    "\n",
    "# CNMF/cleaning\n",
    "K = 3; gSig = np.array([w10 // 2, w10 // 2]); gSiz = 2 * gSig + 1\n",
    "min_SNR = 1.0; rval_thr = 0.3; merge_thr = 0.95\n",
    "threshold_percentile = 15; convolution_threshold = 50\n",
    "min_pixels = ((w10) ** 2) / 4; max_pixels = ((w10) ** 2) * 6\n",
    "\n",
    "# Export paths\n",
    "export_hdf5 = out_dir / \"batch_analysis_results.hdf5\"\n",
    "sampling_movie_path = out_dir / \"sampling_movie_for_segmentation.tif\"\n",
    "summary_fig_A = out_dir / \"summary_corr_max_overlays.png\"\n",
    "\n",
    "print(\"Parent directory:\", parent_dir.resolve())\n",
    "print(\"Outputs:\", out_dir.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84fe1fa",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "1. Filesystem Times & Helpers\n",
    "2. Ground‑Truth Absolute Timeline (from `.h5`)\n",
    "    - Locate the single `.h5` in `parent_dir`\n",
    "    - Set **`h5_start_time_wall`** to filesystem birth time (or mtime fallback)\n",
    "    - Read **ImagingWindow TTL** and parse **rising edges** at **5 kHz**\n",
    "    - Convert onsets to **absolute** timestamps by adding anchor time\n",
    "3. Timing helpers\n",
    "    - Get actual timestamps from raw tiffs\n",
    "    - Align to h5 trial onsets\n",
    "    - Map trials to motion corrected videos\n",
    "4. Concatenate and parse behavioral data\n",
    "    - Order behavior files\n",
    "    - Concatenate all `*_Events.csv` into a single timeline and then compute **trial outcomes** between the first `Response` and the subsequent `Trial Interval`:\n",
    "        - If any `Reward R` or `Reward L` occurs in the window → **Hit**  \n",
    "        - Else if `Lick L` + `Lick R` count ≥ 3 → **False Choice**  \n",
    "        - Else → **Miss**\n",
    "4. CNMF helpers\n",
    "5. Helpers for building and saving master output `h5` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21146dd0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_birth_or_mtime_seconds(path: Path) -> float:\n",
    "    \"\"\"Return filesystem creation time if available, else mtime, as POSIX seconds (UTC).\"\"\"\n",
    "    st = path.stat()\n",
    "    t = getattr(st, 'st_birthtime', None)\n",
    "    if t is None:\n",
    "        t = st.st_mtime\n",
    "    return float(t)\n",
    "\n",
    "\n",
    "def as_utc_datetime(ts_seconds: float) -> datetime:\n",
    "    return datetime.fromtimestamp(ts_seconds, tz=timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a109d75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _get_birth_or_mtime_seconds(path: Path) -> float:\n",
    "    st = path.stat()\n",
    "    return float(getattr(st, \"st_birthtime\", st.st_mtime))\n",
    "\n",
    "def _as_utc(ts_seconds: float) -> datetime:\n",
    "    return datetime.fromtimestamp(ts_seconds, tz=timezone.utc)\n",
    "\n",
    "def _find_single_h5(parent_dir: Path) -> Path:\n",
    "    h5s = list(parent_dir.glob(\"*.h5\"))\n",
    "    if len(h5s) != 1:\n",
    "        raise FileNotFoundError(f\"Expected exactly one .h5 in {parent_dir}, found {len(h5s)}\")\n",
    "    return h5s[0]\n",
    "\n",
    "def _find_imagingwindow_dataset(h5f: h5py.File) -> str:\n",
    "    hits = []\n",
    "    def _cb(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset) and \"imagingwindow\" in name.lower():\n",
    "            hits.append(name)\n",
    "    h5f.visititems(_cb)\n",
    "    if not hits:\n",
    "        raise KeyError(\"Could not find a dataset containing 'ImagingWindow' in the .h5\")\n",
    "    return hits[0]\n",
    "\n",
    "def _percentile_threshold(x: np.ndarray, q_low: float, q_high: float, frac: float) -> float:\n",
    "    lo = float(np.percentile(x, q_low))\n",
    "    hi = float(np.percentile(x, q_high))\n",
    "    if not np.isfinite(hi - lo) or hi <= lo:\n",
    "        lo, hi = float(np.min(x)), float(np.max(x))\n",
    "    return lo + float(frac) * (hi - lo)\n",
    "\n",
    "def build_ground_truth_trial_onsets(\n",
    "    parent_dir: Path,\n",
    "    fs_hz: float = 5000.0,\n",
    "    dataset_hint: str | None = None,   # e.g., \"Acquisition/ImagingWindow\"\n",
    "    # Rising-edge policy\n",
    "    min_low_s: float = 5.0,            # require ≥ this much time since last kept edge\n",
    "    debounce_ms: float = 10.0,         # ignore edges within this many ms of the previous raw edge\n",
    "    # Threshold policy on RAW signal (fixed polarity: HIGH when ttl >= thr)\n",
    "    threshold_method: str = \"percentile\",  # \"percentile\" | \"manual\"\n",
    "    q_low: float = 1.0, q_high: float = 99.99, thr_frac: float = 0.50,\n",
    "    manual_threshold: float | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        h5_path (Path),\n",
    "        h5_start_time_wall (UTC datetime),\n",
    "        trial_onsets_wall_posix (np.ndarray of float POSIX seconds)\n",
    "    \"\"\"\n",
    "    # 0) Anchor\n",
    "    h5_path = _find_single_h5(parent_dir)\n",
    "    anchor_posix = _get_birth_or_mtime_seconds(h5_path)\n",
    "    h5_start_time_wall = _as_utc(anchor_posix)\n",
    "\n",
    "    # 1) Load raw TTL\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        ds_path = dataset_hint if dataset_hint else _find_imagingwindow_dataset(f)\n",
    "        ttl = np.asarray(f[ds_path], dtype=float).ravel()\n",
    "        if ttl.ndim != 1 or ttl.size < 3:\n",
    "            raise ValueError(f\"'ImagingWindow' dataset must be a 1D TTL-like trace. Got shape {ttl.shape}\")\n",
    "\n",
    "    # 2) Threshold (fixed polarity)\n",
    "    if threshold_method == \"manual\" and (manual_threshold is not None):\n",
    "        thr = float(manual_threshold)\n",
    "        thr_info = f\"manual thr={thr:.6g}\"\n",
    "    else:\n",
    "        thr = _percentile_threshold(ttl, q_low=q_low, q_high=q_high, frac=thr_frac)\n",
    "        thr_info = f\"percentile p{q_low}/{q_high} frac {thr_frac:.2f} → thr={thr:.6g}\"\n",
    "\n",
    "    high = ttl >= thr\n",
    "\n",
    "    # 3) Rising edges on raw thresholded signal (low→high)\n",
    "    rising_idx = np.flatnonzero(~high[:-1] & high[1:]) + 1\n",
    "    n_raw_edges = int(rising_idx.size)\n",
    "\n",
    "    # Optional small debounce to collapse any double-detects from noisy shoulders\n",
    "    if n_raw_edges:\n",
    "        min_sep = max(1, int(round((debounce_ms / 1000.0) * fs_hz)))\n",
    "        kept = [int(rising_idx[0])]\n",
    "        for idx in rising_idx[1:]:\n",
    "            if idx - kept[-1] >= min_sep:\n",
    "                kept.append(int(idx))\n",
    "        rising_idx = np.asarray(kept, dtype=int)\n",
    "\n",
    "    # 4) Keep first edge then require ≥ min_low_s gap to keep the next (greedy)\n",
    "    if rising_idx.size == 0:\n",
    "        rel_onsets_s = np.array([], dtype=float)\n",
    "    else:\n",
    "        gap_samples = int(round(min_low_s * fs_hz))\n",
    "        kept = [int(rising_idx[0])]\n",
    "        for idx in rising_idx[1:]:\n",
    "            if idx - kept[-1] >= gap_samples:\n",
    "                kept.append(int(idx))\n",
    "        rel_onsets_s = np.asarray(kept, dtype=float) / float(fs_hz)\n",
    "\n",
    "    # 5) Absolute wall time\n",
    "    trial_onsets_wall_posix = anchor_posix + rel_onsets_s\n",
    "\n",
    "    # Diagnostics\n",
    "    iti = np.diff(rel_onsets_s) if rel_onsets_s.size > 1 else np.array([])\n",
    "    print(f\"[Ground Truth] .h5: {h5_path.name}\")\n",
    "    print(f\"[Ground Truth] Using '{ds_path}' (fs={fs_hz} Hz)\")\n",
    "    print(f\"[Ground Truth] {thr_info}\")\n",
    "    print(f\"[Ground Truth] rising edges (raw): {n_raw_edges}  → after debounce: {len(trial_onsets_wall_posix)} (before gap filter: {rising_idx.size})\")\n",
    "    print(f\"[Ground Truth] min_low_s (gap keep rule): {min_low_s:.2f}s  | debounce={debounce_ms:.1f}ms\")\n",
    "    print(f\"[Ground Truth] Trials kept: {len(trial_onsets_wall_posix)}\")\n",
    "    if iti.size:\n",
    "        print(f\"[Ground Truth] ITI (s): mean={iti.mean():.2f}, median={np.median(iti):.2f}, \"\n",
    "              f\"min={iti.min():.2f}, max={iti.max():.2f}\")\n",
    "    print(f\"[Ground Truth] h5_start_time_wall (UTC): {h5_start_time_wall.isoformat()}\")\n",
    "\n",
    "    return h5_path, h5_start_time_wall, trial_onsets_wall_posix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381860a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- Config --------\n",
    "DEFAULT_LOCAL_TZ = \"America/Los_Angeles\"  # set to your acquisition timezone\n",
    "\n",
    "# -------- Basics --------\n",
    "def get_valid_tiff_files(directory: Path):\n",
    "    \"\"\"List *.tif/*.tiff (excluding '._' files), sorted by name.\"\"\"\n",
    "    all_files = sorted(list(directory.glob(\"*.tif\")) + list(directory.glob(\"*.tiff\")))\n",
    "    return [f for f in all_files if not f.name.startswith(\"._\")]\n",
    "\n",
    "def _file_birth_or_mtime_seconds(p: Path) -> float:\n",
    "    st = p.stat()\n",
    "    t = getattr(st, \"st_birthtime\", None)\n",
    "    if t is None:\n",
    "        t = st.st_mtime\n",
    "    return float(t)\n",
    "\n",
    "# -------- ScanImage ImageDescription parsing --------\n",
    "_RE_EPOCH = re.compile(\n",
    "    r\"epoch\\s*=\\s*\\[\\s*(\\d{4})\\s+(\\d{1,2})\\s+(\\d{1,2})\\s+(\\d{1,2})\\s+(\\d{1,2})\\s+([\\d\\.]+)\\s*\\]\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "def _re_float_field(name: str):\n",
    "    return re.compile(rf\"{name}\\s*=\\s*([\\-+]?\\d+(?:\\.\\d+)?)\", re.IGNORECASE)\n",
    "\n",
    "_RE_FRAME_TS = _re_float_field(\"frameTimestamps_sec\")\n",
    "_RE_ACQ_TRIG = _re_float_field(\"acqTriggerTimestamps_sec\")\n",
    "\n",
    "def _first_frame_wall_time_from_desc(tif_path: Path, local_tz: str = DEFAULT_LOCAL_TZ) -> float | None:\n",
    "    \"\"\"\n",
    "    First-frame POSIX UTC seconds using:\n",
    "      epoch (LOCAL) + frameTimestamps_sec  [preferred]\n",
    "      epoch (LOCAL) + acqTriggerTimestamps_sec  [fallback]\n",
    "    Returns None if required fields are missing.\n",
    "    \"\"\"\n",
    "    with tifffile.TiffFile(tif_path) as tf:\n",
    "        desc = getattr(tf.pages[0], \"description\", \"\") or \"\"\n",
    "    m_epoch = _RE_EPOCH.search(desc)\n",
    "    if not m_epoch:\n",
    "        return None\n",
    "\n",
    "    Y, M, D, h, m, s_float = m_epoch.groups()\n",
    "    Y, M, D, h, m = map(int, (Y, M, D, h, m))\n",
    "    s_float = float(s_float)\n",
    "\n",
    "    # Build timezone-aware local epoch, then add relative seconds\n",
    "    epoch_local = pd.Timestamp(year=Y, month=M, day=D, hour=h, minute=m, second=0, tz=local_tz)\n",
    "    epoch_local = epoch_local + pd.to_timedelta(s_float, unit=\"s\")\n",
    "\n",
    "    m_frame = _RE_FRAME_TS.search(desc)\n",
    "    m_trig  = _RE_ACQ_TRIG.search(desc)\n",
    "    if m_frame:\n",
    "        rel = float(m_frame.group(1))\n",
    "    elif m_trig:\n",
    "        rel = float(m_trig.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    first_local = epoch_local + pd.to_timedelta(rel, unit=\"s\")\n",
    "    first_utc = first_local.tz_convert(\"UTC\")\n",
    "    return float(first_utc.value / 1e9)\n",
    "\n",
    "# -------- List RAW TIFF first-frame times --------\n",
    "def list_tiff_onsets_raw(\n",
    "    raw_dir: Path,\n",
    "    local_tz: str = DEFAULT_LOCAL_TZ,\n",
    "    fallback_filesystem: bool = True,\n",
    "    verbose_every: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a time-sorted list of (filename_stem, first_frame_posix_utc_seconds).\n",
    "\n",
    "    Primary source: ScanImage ImageDescription\n",
    "      - epoch = [YYYY M D h m s.sss]   (LOCAL time base)\n",
    "      - frameTimestamps_sec            (preferred)\n",
    "      - acqTriggerTimestamps_sec       (fallback)\n",
    "    Optional fallback: filesystem birth/mtime if metadata missing.\n",
    "    \"\"\"\n",
    "    tiffs = get_valid_tiff_files(raw_dir)\n",
    "    if not tiffs:\n",
    "        raise FileNotFoundError(f\"No TIFFs in raw_dir: {raw_dir}\")\n",
    "\n",
    "    records: list[tuple[str, float]] = []\n",
    "    missing_meta = 0\n",
    "\n",
    "    for i, p in enumerate(tiffs):\n",
    "        t0 = _first_frame_wall_time_from_desc(p, local_tz=local_tz)\n",
    "        if t0 is None:\n",
    "            missing_meta += 1\n",
    "            if fallback_filesystem:\n",
    "                t0 = _file_birth_or_mtime_seconds(p)\n",
    "            else:\n",
    "                # skip files with no usable metadata if no fallback\n",
    "                continue\n",
    "        records.append((p.stem, float(t0)))\n",
    "        if verbose_every and (i % verbose_every == 0):\n",
    "            print(f\"[RAW] {p.name}: {pd.to_datetime(t0, unit='s', utc=True).isoformat()}\")\n",
    "\n",
    "    records.sort(key=lambda x: x[1])\n",
    "    print(f\"[RAW] extracted first-frame times for {len(records)} raw TIFFs \"\n",
    "          f\"(metadata missing on {missing_meta}{' — used file-time fallback' if fallback_filesystem and missing_meta else ''})\")\n",
    "    return records\n",
    "\n",
    "# -------- Stems → MCOR paths --------\n",
    "def map_stem_to_mcor_path(mcor_dir: Path):\n",
    "    \"\"\"Map filename stem → full motion-corrected TIFF path.\"\"\"\n",
    "    return {p.stem: p for p in get_valid_tiff_files(mcor_dir)}\n",
    "\n",
    "# -------- Offset, mapping, diagnostics --------\n",
    "def compute_coarse_offset_first(\n",
    "    ground_truth_onsets_posix: np.ndarray,\n",
    "    raw_onsets_records: list[tuple[str, float]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Coarse offset to align H5 → RAW, using first items only:\n",
    "      offset_s = first_raw_first_frame - first_h5_onset\n",
    "    Add this offset to all H5 onsets before matching.\n",
    "    \"\"\"\n",
    "    if len(raw_onsets_records) == 0:\n",
    "        raise ValueError(\"raw_onsets_records is empty.\")\n",
    "    if ground_truth_onsets_posix is None or len(ground_truth_onsets_posix) == 0:\n",
    "        raise ValueError(\"ground_truth_onsets_posix is empty.\")\n",
    "    first_raw_t = float(raw_onsets_records[0][1])\n",
    "    first_h5_t  = float(ground_truth_onsets_posix[0])\n",
    "    return first_raw_t - first_h5_t\n",
    "\n",
    "def shift_onsets(onsets_posix: np.ndarray, offset_s: float) -> np.ndarray:\n",
    "    \"\"\"Return a shifted copy of onsets: onsets' = onsets + offset_s.\"\"\"\n",
    "    a = np.asarray(onsets_posix, dtype=float)\n",
    "    return a + float(offset_s)\n",
    "\n",
    "def build_trial_to_filename_map_monotonic(\n",
    "    gt_onsets_posix: np.ndarray,\n",
    "    raw_onsets_records: list[tuple[str, float]],\n",
    "    tolerance_s: float\n",
    ") -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Greedy, left→right, one-to-one nearest mapping with tolerance.\n",
    "    Returns {trial_index: raw_stem}.\n",
    "    \"\"\"\n",
    "    stems = [s for s, _ in raw_onsets_records]\n",
    "    times = np.array([t for _, t in raw_onsets_records], dtype=float)\n",
    "    mapping: dict[int, str] = {}\n",
    "    j = 0\n",
    "    for i, gt in enumerate(np.asarray(gt_onsets_posix, dtype=float)):\n",
    "        # advance while the next is closer\n",
    "        while j + 1 < len(times) and abs(times[j + 1] - gt) <= abs(times[j] - gt):\n",
    "            j += 1\n",
    "        if j < len(times) and abs(times[j] - gt) <= tolerance_s:\n",
    "            mapping[i] = stems[j]\n",
    "            j += 1  # consume this raw entry to keep one-to-one\n",
    "    return mapping\n",
    "\n",
    "def print_alignment_diagnostics(\n",
    "    mapping: dict[int, str],\n",
    "    raw_onsets_records: list[tuple[str, float]],\n",
    "    used_gt_onsets_posix: np.ndarray,\n",
    "    max_examples: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Print Δt stats (RAW − H5') for the accepted matches and show a few examples.\n",
    "    \"\"\"\n",
    "    if not mapping:\n",
    "        print(\"[Align] No matches to report.\")\n",
    "        return\n",
    "    raw_times = dict(raw_onsets_records)\n",
    "    deltas_ms = np.array([(raw_times[stem] - float(used_gt_onsets_posix[i])) * 1000.0\n",
    "                          for i, stem in mapping.items()], dtype=float)\n",
    "    print(f\"[Align] {len(mapping)} matches | Δt (ms): \"\n",
    "          f\"median={np.median(deltas_ms):+.1f}, mean={np.mean(deltas_ms):+.1f}, \"\n",
    "          f\"min={np.min(deltas_ms):+.1f}, max={np.max(deltas_ms):+.1f}\")\n",
    "    for k, (i, stem) in enumerate(list(mapping.items())[:max_examples]):\n",
    "        t_raw = raw_times[stem]\n",
    "        t_gt  = float(used_gt_onsets_posix[i])\n",
    "        print(f\"  trial {i:3d}  RAW={datetime.fromtimestamp(t_raw, tz=timezone.utc).isoformat()}  \"\n",
    "              f\"H5'={datetime.fromtimestamp(t_gt,  tz=timezone.utc).isoformat()}  \"\n",
    "              f\"Δ={(t_raw - t_gt)*1000:+.1f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900693a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_trial_to_filename_map(gt_onsets_wall_sec: np.ndarray,\n",
    "                                raw_onsets: list[tuple[str, float]],\n",
    "                                tolerance_s: float = 2.0) -> dict[int, str]:\n",
    "    if len(gt_onsets_wall_sec) == 0 or len(raw_onsets) == 0:\n",
    "        return {}\n",
    "    raw_times = np.array([t for (_, t) in raw_onsets], dtype=float)\n",
    "    mapping = {}\n",
    "    for i, t_gt in enumerate(gt_onsets_wall_sec):\n",
    "        j = int(np.argmin(np.abs(raw_times - t_gt)))\n",
    "        dt = abs(raw_times[j] - t_gt)\n",
    "        if dt <= tolerance_s:\n",
    "            mapping[i] = raw_onsets[j][0]\n",
    "    print(f\"[Align] Mapped {len(mapping)} / {len(gt_onsets_wall_sec)} trials (tol={tolerance_s}s)\")\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# MCOR resolution by trailing number only\n",
    "\n",
    "def extract_trailing_number(stem: str) -> tuple[str | None, int | None]:\n",
    "    \"\"\"\n",
    "    Extract the final numeric token from a filename stem, allowing a trailing `_mcor`.\n",
    "    Examples:\n",
    "      '20250822_sid174_e1_00037'        -> ('00037', 37)\n",
    "      '20250822_sid174_e1_00037_mcor'   -> ('00037', 37)\n",
    "    Returns (string_token, int_value) or (None, None) if no number found.\n",
    "    \"\"\"\n",
    "    # strip an optional trailing '_mcor'\n",
    "    s = re.sub(r'(_mcor)$', '', stem, flags=re.IGNORECASE)\n",
    "    # take the numeric token at the end (after removing _mcor)\n",
    "    m = re.search(r'(\\d+)$', s)\n",
    "    if not m:\n",
    "        # fallback: take the last numeric block anywhere\n",
    "        m = re.search(r'(\\d+)(?!.*\\d)', s)\n",
    "        if not m:\n",
    "            return None, None\n",
    "    tok = m.group(1)\n",
    "    try:\n",
    "        return tok, int(tok)\n",
    "    except Exception:\n",
    "        return tok, None\n",
    "\n",
    "def map_trailing_number_to_mcor_path(mcor_dir: Path, prefer_shortest_name: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Build an index from trailing number -> MCOR path.\n",
    "    Keys include both the zero-padded string and the integer:\n",
    "      idx['00037'] -> Path(...)\n",
    "      idx[37]      -> Path(...)\n",
    "    If multiple files map to the same number, choose the shortest filename.\n",
    "    \"\"\"\n",
    "    idx: dict[object, Path] = {}\n",
    "    for p in get_valid_tiff_files(mcor_dir):\n",
    "        tok, num = extract_trailing_number(p.stem)\n",
    "        if tok is None:\n",
    "            continue\n",
    "        for key in (tok, num):\n",
    "            if key is None:\n",
    "                continue\n",
    "            if key not in idx:\n",
    "                idx[key] = p\n",
    "            elif prefer_shortest_name and len(p.name) < len(idx[key].name):\n",
    "                idx[key] = p\n",
    "    return idx\n",
    "\n",
    "def resolve_mcor_for_trials_by_number(trial_to_stem: dict[int, str], mcor_dir: Path):\n",
    "    \"\"\"\n",
    "    Resolve each aligned RAW stem to an MCOR file using ONLY the number token.\n",
    "    Returns (resolved_map, unresolved_list).\n",
    "    \"\"\"\n",
    "    idx = map_trailing_number_to_mcor_path(mcor_dir)\n",
    "    resolved: dict[int, Path] = {}\n",
    "    unresolved: list[tuple[int, str]] = []\n",
    "    for i, raw_stem in trial_to_stem.items():\n",
    "        tok, num = extract_trailing_number(raw_stem)  # RAW stems already end with the number\n",
    "        p = idx.get(tok) or idx.get(num)\n",
    "        if p is None:\n",
    "            unresolved.append((i, raw_stem))\n",
    "        else:\n",
    "            resolved[i] = p\n",
    "    return resolved, unresolved\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e3064",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_behavior_files(parent_dir: Path):\n",
    "    print(\"--- Searching for behavioral event files ---\")\n",
    "    behavior_dirs = list(parent_dir.glob('????_??_??-??_??_??'))\n",
    "    if not behavior_dirs:\n",
    "        raise FileNotFoundError(f\"No date-stamped behavior directory in {parent_dir}\")\n",
    "    behavior_base_dir = behavior_dirs[0]\n",
    "    print(f\"  - Found behavior directory: {behavior_base_dir.name}\")\n",
    "    protocol_dirs = sorted([d for d in behavior_base_dir.iterdir() if d.is_dir()])\n",
    "    all_events_csvs = []\n",
    "    for protocol_dir in protocol_dirs:\n",
    "        events_in_protocol = sorted(list(protocol_dir.glob('*Events.csv')))\n",
    "        all_events_csvs.extend(events_in_protocol)\n",
    "    if not all_events_csvs:\n",
    "        raise FileNotFoundError(f\"No '*Events.csv' files in {behavior_base_dir}\")\n",
    "    print(f\"  - Found and sorted {len(all_events_csvs)} Events CSV files across {len(protocol_dirs)} protocols.\")\n",
    "    return {'events_csvs': all_events_csvs}\n",
    "\n",
    "\n",
    "def _parse_trial_slice(trial_df, event_col, time_col):\n",
    "    odor_event_series = trial_df[trial_df[event_col].str.startswith('Odor', na=False)]\n",
    "    odor_id = -1\n",
    "    if not odor_event_series.empty:\n",
    "        odor_event_string = odor_event_series[event_col].iloc[0]\n",
    "        match = re.search(r'Odor I (\\d+)', odor_event_string)\n",
    "        if match:\n",
    "            odor_id = int(match.group(1))\n",
    "    lick_events = trial_df[trial_df[event_col] == 'Lick']\n",
    "    reward_events = trial_df[trial_df[event_col] == 'Reward']\n",
    "    outcome = 'Miss'\n",
    "    if not reward_events.empty:\n",
    "        reward_value = str(reward_events.get('Value', pd.Series([\"\"])).iloc[0]).lower()\n",
    "        if 'r' in reward_value: outcome = 'Reward R'\n",
    "        elif 'l' in reward_value: outcome = 'Reward L'\n",
    "        else: outcome = 'Reward'\n",
    "    elif not lick_events.empty:\n",
    "        outcome = 'FC'\n",
    "    return {'odor_id': odor_id, 'outcome': outcome}\n",
    "\n",
    "\n",
    "def process_and_concatenate_events(events_csv_files, odor_duration, time_col, event_col):\n",
    "    \"\"\"\n",
    "    Outcome logic:\n",
    "      - Define window = [first 'Response', first 'Trial Interval' after it (or trial end)]\n",
    "      - If any 'Reward R' or 'Reward L' in window  -> 'Hit'\n",
    "      - Else if count('Lick L' + 'Lick R') >= 3    -> 'False Choice'\n",
    "      - Else                                        -> 'Miss'\n",
    "    \"\"\"\n",
    "    print(\"--- Concatenating and processing all behavioral events ---\")\n",
    "    processed_dfs = []\n",
    "    cumulative_time_offset = 0.0\n",
    "\n",
    "    # 1) Read and stitch timelines into a single continuous stream\n",
    "    for file_path in events_csv_files:\n",
    "        df = pd.read_csv(file_path, header=1)\n",
    "        if time_col not in df.columns:\n",
    "            raise KeyError(f\"Time column '{time_col}' not found in {Path(file_path).name}. Headers: {list(df.columns)}\")\n",
    "        if event_col not in df.columns:\n",
    "            raise KeyError(f\"Event column '{event_col}' not found in {Path(file_path).name}. Headers: {list(df.columns)}\")\n",
    "\n",
    "        df[time_col] = pd.to_numeric(df[time_col], errors='coerce').fillna(0.0)\n",
    "        df[time_col] += cumulative_time_offset\n",
    "        processed_dfs.append(df)\n",
    "\n",
    "        # next file starts after this one’s max time\n",
    "        cumulative_time_offset = float(df[time_col].max())\n",
    "\n",
    "    master_events_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    print(f\"  - Continuous timeline length: {cumulative_time_offset:.2f}s\")\n",
    "\n",
    "    # 2) Split into trials by odor onset marker ('Output 1')\n",
    "    trial_starts = master_events_df[master_events_df[event_col] == 'Output 1']\n",
    "    trial_summary_list = []\n",
    "\n",
    "    # Classify outcome using window [Response, Trial Interval)\n",
    "    def classify_outcome(trial_df, start_time, end_time):\n",
    "        # Find first Response in this trial\n",
    "        resp_rows = trial_df[trial_df[event_col] == 'Response']\n",
    "        if resp_rows.empty:\n",
    "            # No response window -> cannot be Hit; treat as Miss (no/too-few licks by definition)\n",
    "            return \"Miss\"\n",
    "\n",
    "        response_t = float(resp_rows.iloc[0][time_col])\n",
    "\n",
    "        # Window ends at first Trial Interval after Response, else trial end\n",
    "        ti_after_resp = trial_df[(trial_df[time_col] > response_t) & (trial_df[event_col] == 'Trial Interval')]\n",
    "        window_end = float(ti_after_resp.iloc[0][time_col]) if not ti_after_resp.empty else end_time\n",
    "\n",
    "        win = trial_df[(trial_df[time_col] >= response_t) & (trial_df[time_col] < window_end)]\n",
    "\n",
    "        # Reward?\n",
    "        if win[event_col].isin(['Reward R', 'Reward L']).any():\n",
    "            return \"Hit\"\n",
    "\n",
    "        # Licks?\n",
    "        lick_count = int(win[event_col].isin(['Lick R', 'Lick L']).sum())\n",
    "        if lick_count >= 3:\n",
    "            return \"False Choice\"\n",
    "        else:\n",
    "            return \"Miss\"\n",
    "\n",
    "    # 3) Build per-trial summary rows\n",
    "    for i in range(len(trial_starts)):\n",
    "        start_time = float(trial_starts.iloc[i][time_col])\n",
    "        end_time = float(trial_starts.iloc[i+1][time_col]) if i + 1 < len(trial_starts) else float(master_events_df[time_col].max())\n",
    "\n",
    "        trial_df = master_events_df[\n",
    "            (master_events_df[time_col] >= start_time) & (master_events_df[time_col] < end_time)\n",
    "        ]\n",
    "\n",
    "        parsed_info = _parse_trial_slice(trial_df, event_col, time_col)  # expects keys 'odor_id', maybe old 'outcome'\n",
    "\n",
    "        # Override outcome according to the new rules\n",
    "        outcome = classify_outcome(trial_df, start_time, end_time)\n",
    "\n",
    "        trial_summary_list.append({\n",
    "            'trial_index'    : i,\n",
    "            'trial_start_time': start_time,\n",
    "            'odor_id'        : parsed_info.get('odor_id', None),\n",
    "            'outcome'        : outcome,\n",
    "            'trial_end_time' : end_time\n",
    "        })\n",
    "\n",
    "    trial_summary_df = pd.DataFrame(trial_summary_list)\n",
    "    print(f\"  - Parsed {len(trial_summary_df)} trials.\")\n",
    "    return master_events_df, trial_summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ddd75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_scanimage_fps(movie_path, default_fr=None):\n",
    "    try:\n",
    "        with tifffile.TiffFile(movie_path) as tif:\n",
    "            if not hasattr(tif, 'scanimage_metadata'):\n",
    "                return default_fr\n",
    "            si_meta = tif.scanimage_metadata\n",
    "            possible_keys = ['SI.hRoiManager.scanFrameRate', 'SI.hScan2D.scanFrameRate', 'hScan2D.scanFrameRate']\n",
    "            frame_data = si_meta.get('FrameData', {})\n",
    "            for key in possible_keys:\n",
    "                if key in frame_data:\n",
    "                    return float(frame_data[key])\n",
    "            return default_fr\n",
    "    except Exception:\n",
    "        return default_fr\n",
    "\n",
    "\n",
    "def create_sampling_movie(mcor_dir: Path, output_path: Path, num_frames_per_trial: int, fr: float):\n",
    "    all_frames = []\n",
    "    trial_files_list = get_valid_tiff_files(mcor_dir)\n",
    "    trial_files = trial_files_list[::3]\n",
    "    if not trial_files:\n",
    "        raise FileNotFoundError(f\"No valid TIFF files found in {mcor_dir}\")\n",
    "    print(f\"Found {len(trial_files)} trial videos. Sampling frames from 5-7s window...\")\n",
    "    for trial_path in tqdm(trial_files, desc=\"Sampling trials\"):\n",
    "        with tifffile.TiffFile(trial_path) as tif:\n",
    "            num_total_frames = len(tif.series[0].pages)\n",
    "            start_frame, end_frame = int(5.0 * fr), int(7.0 * fr)\n",
    "            end_frame = min(end_frame, num_total_frames)\n",
    "            if start_frame >= end_frame:\n",
    "                continue\n",
    "            valid_indices_pool = np.arange(start_frame, end_frame)\n",
    "            num_to_sample = min(num_frames_per_trial, len(valid_indices_pool))\n",
    "            if num_to_sample == 0:\n",
    "                continue\n",
    "            frame_indices = np.random.choice(valid_indices_pool, size=num_to_sample, replace=False)\n",
    "            all_frames.append(tif.asarray(key=frame_indices))\n",
    "    if not all_frames:\n",
    "        raise ValueError(\"No frames were sampled. Check if videos are shorter than 7s.\")\n",
    "    sampling_movie = np.concatenate(all_frames, axis=0)\n",
    "    tifffile.imwrite(output_path, sampling_movie)\n",
    "    print(f\"Saved sampling movie with {sampling_movie.shape[0]} frames to: {output_path}\")\n",
    "    return output_path, sampling_movie\n",
    "\n",
    "\n",
    "def build_cnmf_params(movie_path_str, frame_rate, dxy_tuple):\n",
    "    rf_target, stride_target = int(w10 + 2), int(2 * w10 / 3)\n",
    "    opts_dict = {\n",
    "        \"data\": {\"fnames\": [movie_path_str], \"fr\": float(frame_rate), \"dxy\": tuple(map(float, dxy_tuple))},\n",
    "        \"decay_time\": decay_time,\n",
    "        \"patch\": {\"rf\": rf_target, \"stride\": stride_target, \"only_init\": False},\n",
    "        \"init\": {\"K\": int(K), \"gSig\": [int(gSig[0]), int(gSig[1])], \"gSiz\": [int(gSiz[0]), int(gSiz[1])], \"ssub\": 1, \"tsub\": 1},\n",
    "        \"merging\": {\"merge_thr\": merge_thr},\n",
    "        \"quality\": {\"min_SNR\": float(min_SNR), \"rval_thr\": float(rval_thr)}\n",
    "    }\n",
    "    return caiman_params.CNMFParams(params_dict=opts_dict)\n",
    "\n",
    "\n",
    "def fit_cnmf(images, params, n_processes=1, dview=None):\n",
    "    info = {\"used_fit_file\": True, \"did_refit\": False, \"delta_C_rms\": None}\n",
    "    print(\"  - Converting sampling movie to CaImAn's memmap format...\")\n",
    "    caiman_mmap_path = cm.save_memmap([images], base_name='caiman_refit_movie_', order='C')\n",
    "    model = cnmf_mod.CNMF(n_processes=n_processes, params=params, dview=dview)\n",
    "    model.fit_file(caiman_mmap_path)\n",
    "    print(f\"[CNMF] Components after initial fit: {model.estimates.A.shape[1]}\")\n",
    "    C_before = np.array(model.estimates.C, copy=True)\n",
    "    try:\n",
    "        print(\"  - Refitting components...\")\n",
    "        images_for_refit = cm.load(caiman_mmap_path)\n",
    "        images2 = np.array(images_for_refit, dtype=np.float32, order=\"F\", copy=True)\n",
    "        np.nan_to_num(images2, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        cnmf_refit = model.refit(images2)\n",
    "        info[\"did_refit\"] = True\n",
    "        C_after = np.array(cnmf_refit.estimates.C, copy=True)\n",
    "        if C_before.shape == C_after.shape:\n",
    "            diff = C_after - C_before\n",
    "            rms = float(np.sqrt(np.mean(diff**2)))\n",
    "            info[\"delta_C_rms\"] = rms\n",
    "            print(f\"  - Refit ΔC RMS: {rms:.6g}\")\n",
    "        print(f\"[CNMF] Components after refit: {cnmf_refit.estimates.A.shape[1]}\")\n",
    "        return cnmf_refit, info\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: refit failed: {e}. Proceeding with un-refit components.\")\n",
    "        return model, info\n",
    "\n",
    "\n",
    "def clean_components(fit, **kwargs):\n",
    "    d1, d2 = fit.dims\n",
    "    A_full = fit.estimates.A.toarray().reshape((d1, d2, -1), order=\"F\")\n",
    "    kept_indices, contours = [], {'valid':[], 'large':[], 'small':[], 'convoluted':[]}\n",
    "    for k in range(A_full.shape[2]):\n",
    "        comp = A_full[:, :, k]\n",
    "        binary = comp > (kwargs['threshold_percentile'] / 100.0) * comp.max()\n",
    "        labeled, n = scipy.ndimage.label(binary)\n",
    "        if n == 0:\n",
    "            continue\n",
    "        mask = (labeled == (np.argmax(np.bincount(labeled.flat)[1:]) + 1))\n",
    "        mask = scipy.ndimage.binary_fill_holes(mask)\n",
    "        area = mask.sum()\n",
    "        if area == 0:\n",
    "            continue\n",
    "        props = regionprops(mask.astype(int))[0]\n",
    "        edge = scipy.ndimage.binary_dilation(mask) ^ mask\n",
    "        if (props.perimeter ** 2) / props.area > kwargs['convolution_threshold']:\n",
    "            contours['convoluted'].append(edge); continue\n",
    "        if area > kwargs['max_pixels']:\n",
    "            contours['large'].append(edge); continue\n",
    "        if area < kwargs['min_pixels']:\n",
    "            contours['small'].append(edge); continue\n",
    "        kept_indices.append(k); contours['valid'].append(edge)\n",
    "    A_cleaned_flat = [fit.estimates.A.toarray()[:, k] for k in kept_indices]\n",
    "    print(f\"Cleaned components: kept={len(contours['valid'])}, small={len(contours['small'])}, large={len(contours['large'])}, convoluted={len(contours['convoluted'])}\")\n",
    "    return {'kept_indices': kept_indices, 'cleaned_A_list': A_cleaned_flat, 'contours': contours}\n",
    "\n",
    "\n",
    "def build_pixel_dataframe(fit, kept_indices):\n",
    "    d1, d2 = fit.dims\n",
    "    pixel_records = []\n",
    "    A_csr = fit.estimates.A.tocsr()\n",
    "    for comp_id in kept_indices:\n",
    "        col = A_csr[:, comp_id].tocoo()\n",
    "        if col.nnz == 0: continue\n",
    "        pix_idx, w_raw = col.row, col.data.astype(float)\n",
    "        w_norm = w_raw / (w_raw.sum() + 1e-10)\n",
    "        y, x = (pix_idx % d1).astype(int), (pix_idx // d1).astype(int)\n",
    "        pixel_records.append(pd.DataFrame({\"ROI\": comp_id, \"pixel_idx\": pix_idx, \"y\": y, \"x\": x, \"weight_raw\": w_raw, \"weight_norm\": w_norm}))\n",
    "    return pd.concat(pixel_records, ignore_index=True) if pixel_records else pd.DataFrame()\n",
    "\n",
    "\n",
    "def extract_traces_for_trial(trial_movie, cleaned_A_list, dims, fr):\n",
    "    T, (d1, d2) = trial_movie.shape[0], dims\n",
    "    A_cleaned = csr_matrix(np.stack(cleaned_A_list, axis=1))\n",
    "    Yr = trial_movie.reshape((T, -1), order=\"F\").T\n",
    "    F_weighted = A_cleaned.T @ Yr\n",
    "    mean_F = F_weighted.mean(axis=1, keepdims=True)\n",
    "    std_F = F_weighted.std(axis=1, keepdims=True); std_F[std_F == 0] = 1\n",
    "    Z_traces = (F_weighted - mean_F) / std_F\n",
    "    win = min(1000, 2 * (T // 10) + 1)\n",
    "    F0 = pd.DataFrame(F_weighted.T).rolling(win, center=True, min_periods=1).quantile(0.08).to_numpy().T\n",
    "    dFF_traces = (F_weighted - F0) / np.maximum(F0, 1e-6)\n",
    "    return np.asarray(F_weighted), np.asarray(dFF_traces), np.asarray(Z_traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23382e",
   "metadata": {},
   "source": [
    "## 2. Create Sampling Movie\n",
    "\n",
    "Infer frame rate from a RAW TIFF if possible, otherwise use `fr_manual`. Build the sampling movie from **MCOR** videos by extracting n random frames from the period during and immediately after the odor presentation. Change slicing in the mcor file list variable to chnge subset of trials used. More trials used takes much longer to load but gives a better sampling of activity patterns across an experiment. Right now the slicing is hard coded to every third file in the experiemntal/processed/mcor directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25587c75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Infer frame rate from RAW TIFF metadata (ScanImage) if available\n",
    "fr_probe = None\n",
    "raw_probe_files = get_valid_tiff_files(raw_dir)\n",
    "if raw_probe_files:\n",
    "    fr_probe = get_scanimage_fps(raw_probe_files[0], default_fr=fr_manual)\n",
    "fr = fr_probe if fr_probe else fr_manual\n",
    "\n",
    "sampling_movie_path, sampling_movie = create_sampling_movie(mcor_dir, sampling_movie_path, num_random_frames_per_trial, fr)\n",
    "\n",
    "# Precompute QC images\n",
    "print(\"--- Pre-computing QC images ---\")\n",
    "max_proj_img = np.max(np.nan_to_num(sampling_movie), axis=0)\n",
    "corr_img = cm.local_correlations(sampling_movie, swap_dim=False)\n",
    "np.save(out_dir / \"qc_max_projection.npy\", max_proj_img)\n",
    "np.save(out_dir / \"qc_correlation_image.npy\", corr_img)\n",
    "plt.figure(figsize=(6,6)); plt.imshow(corr_img, cmap='viridis'); plt.title('Correlation Image'); plt.axis('off'); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3064c-42ad-4b1c-a819-2b58c388dec8",
   "metadata": {},
   "source": [
    "## 3. Run CNMF\n",
    "\n",
    "Runs an inital fitting on patches. If n frames in the sampling video are < 256, it will throw a ton of warnings you can ignore. After initial fitting it runs a refit on the whole video. After the refit, a backup state is saved so that we can resume post fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274248ca-d06d-44e5-a1a9-2e8700d44122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNMF\n",
    "params = build_cnmf_params(str(sampling_movie_path), fr, dxy)\n",
    "dview = None\n",
    "try:\n",
    "    c, dview, n_processes = setup_cluster(backend='multiprocessing')\n",
    "    print(f\"[CNMF] Cluster with {n_processes} procs\")\n",
    "except Exception as e:\n",
    "    print(f\"Cluster start failed, using single process: {e}\")\n",
    "    n_processes = 1\n",
    "\n",
    "fit, fit_info = fit_cnmf(sampling_movie, params, n_processes=n_processes, dview=dview)\n",
    "if dview is not None:\n",
    "    stop_server(dview=dview)\n",
    "    print(\"[CNMF] Cluster stopped.\")\n",
    "\n",
    "# Backup state (lightweight)\n",
    "import pickle\n",
    "backup_path = out_dir / \"caiman_fit_backup.pkl\"\n",
    "with open(backup_path, 'wb') as f:\n",
    "    pickle.dump({'fit': fit, 'fit_info': fit_info, 'fr': fr, 'out_dir': out_dir, 'parent_dir': parent_dir,\n",
    "                 'odor_duration_s': odor_duration_s, 'behavior_time_col': behavior_time_col, 'behavior_event_col': behavior_event_col}, f)\n",
    "print(\"Saved CNMF state to\", backup_path)\n",
    "\n",
    "del sampling_movie; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82a433",
   "metadata": {},
   "source": [
    "## 4. Clean Components & QC Figures\n",
    "\n",
    "Morphological/shape filters classify components. \n",
    "\n",
    "Two QC/diagnostic plots are saved to the output folder: \n",
    "1. correlation image with kept ROI contours\n",
    "2. Max‑projection with kept and rejected ROI contours overlaid and color coded by rejection reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10345cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_params = {\n",
    "    'threshold_percentile': threshold_percentile,\n",
    "    'min_pixels': min_pixels,\n",
    "    'max_pixels': max_pixels,\n",
    "    'convolution_threshold': convolution_threshold\n",
    "}\n",
    "clean_results = clean_components(fit, **cleaning_params)\n",
    "kept_indices = clean_results['kept_indices']\n",
    "cleaned_A_list = clean_results['cleaned_A_list']\n",
    "df_pixels = build_pixel_dataframe(fit, kept_indices)\n",
    "print(\"--- Kept ROI Pixel Info (head) ---\")\n",
    "display(df_pixels.head())\n",
    "\n",
    "# Summary Figure A\n",
    "max_proj = np.load(out_dir / \"qc_max_projection.npy\")\n",
    "corr_img = np.load(out_dir / \"qc_correlation_image.npy\")\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(corr_img, cmap='viridis'); ax1.set_title('Correlation Image'); ax1.axis('off')\n",
    "ax2.imshow(max_proj, cmap='gray'); ax2.set_title('Max Projection + Kept ROIs'); ax2.axis('off')\n",
    "for contour in clean_results['contours']['valid']:\n",
    "    ax2.contour(contour, colors='white', linewidths=0.75)\n",
    "fig.tight_layout(); fig.savefig(summary_fig_A); plt.show()\n",
    "print(\"Saved:\", summary_fig_A)\n",
    "\n",
    "# Detailed overlay\n",
    "from matplotlib.lines import Line2D\n",
    "fig_detail, ax_detail = plt.subplots(figsize=(8,8))\n",
    "ax_detail.imshow(max_proj, cmap='gray')\n",
    "\n",
    "def _draw(ax, edges, color, lw):\n",
    "    for e in edges:\n",
    "        ax.contour(e.astype(float), levels=[0.5], colors=color, linewidths=lw)\n",
    "\n",
    "_draw(ax_detail, clean_results['contours']['convoluted'], 'red', 0.9)\n",
    "_draw(ax_detail, clean_results['contours']['small'], 'cyan', 0.9)\n",
    "_draw(ax_detail, clean_results['contours']['large'], 'magenta', 0.9)\n",
    "_draw(ax_detail, clean_results['contours']['valid'], 'white', 1.2)\n",
    "legend_elements = [\n",
    "    Line2D([0],[0], color='white', lw=2, label=f\"Kept ({len(clean_results['contours']['valid'])})\"),\n",
    "    Line2D([0],[0], color='red', lw=2, label=f\"Rejected (Convoluted) ({len(clean_results['contours']['convoluted'])})\"),\n",
    "    Line2D([0],[0], color='cyan', lw=2, label=f\"Rejected (Small) ({len(clean_results['contours']['small'])})\"),\n",
    "    Line2D([0],[0], color='magenta', lw=2, label=f\"Rejected (Large) ({len(clean_results['contours']['large'])})\"),\n",
    "]\n",
    "ax_detail.legend(handles=legend_elements, loc='lower right', frameon=True, framealpha=0.7)\n",
    "ax_detail.set_title('Component Cleaning Diagnostics'); ax_detail.axis('off')\n",
    "out_overlay_path = out_dir / \"summary_overlay_all_classes.png\"\n",
    "fig_detail.savefig(out_overlay_path, dpi=250, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved:\", out_overlay_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b8f021",
   "metadata": {},
   "source": [
    "## 5. Aligning behvioral trials to correct mcor tiffs\n",
    "\n",
    "- **Step 1**: derive `ground_truth_trial_onsets` from `.h5`\n",
    "- **Step 2**: extract RAW TIFF creation times\n",
    "- **Step 3**: nearest‑neighbor match with tolerance (default `±250 ms`)\n",
    "- **Step 4**: map trials to corresponding **mcor** tiffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: ground-truth onsets from H5 (unchanged) ----\n",
    "TTL_FS_HZ = 5000.0\n",
    "\n",
    "h5_path, h5_start_time_wall, ground_truth_trial_onsets = build_ground_truth_trial_onsets(\n",
    "    parent_dir,\n",
    "    fs_hz=TTL_FS_HZ,\n",
    "    dataset_hint=None,        # or exact HDF5 dataset path\n",
    "    min_low_s=5.0,            # require 5 s quiet before a trial onset\n",
    "    threshold_method=\"percentile\",\n",
    "    q_low=1.0, q_high=99.99, thr_frac=0.50,\n",
    "    manual_threshold=None\n",
    ")\n",
    "\n",
    "print(f\"[GT] .h5 file: {h5_path.name}\")\n",
    "print(f\"[GT] anchor (UTC): {h5_start_time_wall.isoformat()}\")\n",
    "print(f\"[GT] onsets found: {len(ground_truth_trial_onsets)}\")\n",
    "if len(ground_truth_trial_onsets):\n",
    "    print(\"First few onsets (UTC):\")\n",
    "    for s in [\n",
    "        datetime.fromtimestamp(float(t), tz=timezone.utc).isoformat()\n",
    "        for t in ground_truth_trial_onsets[:min(10, len(ground_truth_trial_onsets))]\n",
    "    ]:\n",
    "        print(\"  \", s)\n",
    "\n",
    "# ---- Step 2: RAW TIFF first-frame wall times (ScanImage metadata) ----\n",
    "# Uses ImageDescription: epoch + frameTimestamps_sec (fallback: acqTriggerTimestamps_sec)\n",
    "# Set your acquisition timezone if needed.\n",
    "TOL_SECONDS = 0.25  # try 0.25s; relax to 0.5s if needed\n",
    "raw_onsets_records = list_tiff_onsets_raw(\n",
    "    raw_dir,\n",
    "    local_tz=\"America/Los_Angeles\",\n",
    "    fallback_filesystem=True,   # if metadata missing, use file time\n",
    "    verbose_every=0\n",
    ")\n",
    "print(f\"[RAW] extracted first-frame times for {len(raw_onsets_records)} raw TIFFs\")\n",
    "\n",
    "# ---- Step 2a (optional): quick RAW timing preview ----\n",
    "# if raw_onsets_records:\n",
    "#    raw_times = np.array([t for _, t in raw_onsets_records], dtype=float)\n",
    "#    raw_stems = [s for s, _ in raw_onsets_records]\n",
    "#    print(f\"[RAW] first-frame time range (UTC): \"\n",
    "#          f\"{datetime.fromtimestamp(raw_times[0], tz=timezone.utc).isoformat()}  →  \"\n",
    "#          f\"{datetime.fromtimestamp(raw_times[-1], tz=timezone.utc).isoformat()}\")\n",
    "#    if len(raw_times) > 1:\n",
    "#        d_raw = np.diff(raw_times)\n",
    "#        print(f\"[RAW] ITI (s): mean={d_raw.mean():.2f}, median={np.median(d_raw):.2f}, \"\n",
    "#              f\"min={d_raw.min():.2f}, max={d_raw.max():.2f}\")\n",
    "#    print(\"First few imaging starts (UTC):\")\n",
    "#    for i in range(min(10, len(raw_onsets_records))):\n",
    "#        print(f\"  {i:3d}  {raw_stems[i]:>24s}  {datetime.fromtimestamp(raw_times[i], tz=timezone.utc).isoformat()}\")\n",
    "\n",
    "# ---- Step 3: compute coarse offset from FIRST TIFF vs FIRST H5 onset, shift, and map ----\n",
    "offset_s = compute_coarse_offset_first(ground_truth_trial_onsets, raw_onsets_records)\n",
    "print(f\"[Offset] coarse (H5 → RAW): {offset_s:+.6f} s  ({offset_s*1000:+.1f} ms)\")\n",
    "print(f\"         H5[0]={datetime.fromtimestamp(float(ground_truth_trial_onsets[0]), tz=timezone.utc).isoformat()}\")\n",
    "print(f\"         RAW[0]={datetime.fromtimestamp(float(raw_onsets_records[0][1]), tz=timezone.utc).isoformat()}\")\n",
    "\n",
    "h5_onsets_shifted = shift_onsets(ground_truth_trial_onsets, offset_s)\n",
    "\n",
    "trial_to_stem = build_trial_to_filename_map_monotonic(\n",
    "    h5_onsets_shifted,\n",
    "    raw_onsets_records,\n",
    "    tolerance_s=TOL_SECONDS\n",
    ")\n",
    "print(f\"[Align] mapped {len(trial_to_stem)} / {len(ground_truth_trial_onsets)} trials within ±{int(TOL_SECONDS*1000)} ms\")\n",
    "\n",
    "# ---- Step 3a: diagnostics on accepted matches ----\n",
    "print_alignment_diagnostics(trial_to_stem, raw_onsets_records, h5_onsets_shifted)\n",
    "\n",
    "# ---- Step 4: resolve RAW → MCOR using trailing number only ----\n",
    "\n",
    "resolved_trial_to_mcor, unresolved = resolve_mcor_for_trials_by_number(trial_to_stem, mcor_dir)\n",
    "\n",
    "print(f\"[MCOR] resolved {len(resolved_trial_to_mcor)} / {len(trial_to_stem)} aligned trials to MCOR files (by trailing number)\")\n",
    "\n",
    "# Show a few examples\n",
    "for k, (i, pth) in enumerate(list(resolved_trial_to_mcor.items())[:10]):\n",
    "    print(f\"  trial {i:3d}  →  {pth.name}\")\n",
    "\n",
    "if unresolved:\n",
    "    print(f\"[MCOR] WARNING: {len(unresolved)} aligned trials did not resolve to MCOR by number:\")\n",
    "    for i, stem in unresolved[:10]:\n",
    "        print(f\"  trial {i:3d}  RAW stem='{stem}'  (no MCOR match found)\")\n",
    "    if len(unresolved) > 10:\n",
    "        print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce951d",
   "metadata": {},
   "source": [
    "## 6a. Behavior Parsing & outcome definitions\n",
    "\n",
    "Concatenate all `*_Events.csv` into a single timeline and then compute **trial outcomes** between the first `Response` and the subsequent `Trial Interval`:\n",
    "- If any `Reward R` or `Reward L` occurs in the window → **Hit**  \n",
    "- Else if `Lick L` + `Lick R` count ≥ 3 → **False Choice**  \n",
    "- Else → **Miss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order and parse behavior information from the olfactometer\n",
    "\n",
    "# --- helpers ---\n",
    "_num_re = re.compile(r'(\\d+)')\n",
    "\n",
    "def first_int_in_name(p: Path):\n",
    "    \"\"\"Return the first integer found in the *stem* of the filename, or None if absent.\"\"\"\n",
    "    m = _num_re.search(p.stem)  # use stem so extensions don't interfere\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def natural_key(p: Path):\n",
    "    \"\"\"Natural sort key: e.g., trial_2 before trial_10 (case-insensitive).\"\"\"\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', p.name)]\n",
    "\n",
    "# 1) Find and order event files by first number in filename\n",
    "behavior_files = find_behavior_files(parent_dir)\n",
    "events_csvs = [Path(p) for p in behavior_files['events_csvs']]\n",
    "\n",
    "info = [(p, first_int_in_name(p)) for p in events_csvs]\n",
    "# Sort: numbered files first (ascending by the number), then no-number files (natural order)\n",
    "info_sorted = sorted(info, key=lambda x: (x[1] is None, x[1] if x[1] is not None else float('inf'), natural_key(x[0])))\n",
    "\n",
    "events_csvs_ordered = [p for p, n in info_sorted]\n",
    "\n",
    "# Print the order BEFORE parsing\n",
    "print(\"\\n--- Event files to be parsed (by first number in filename) ---\")\n",
    "for k, (p, n) in enumerate(info_sorted):\n",
    "    tag = f\"n={n}\" if n is not None else \"n=∅\"\n",
    "    print(f\"{k:03d} | {tag} | {p}\")\n",
    "\n",
    "# 2) Parse in that exact order\n",
    "master_events_df, trial_summary_df = process_and_concatenate_events(\n",
    "    events_csv_files=events_csvs_ordered,\n",
    "    odor_duration=odor_duration_s,\n",
    "    time_col=behavior_time_col,\n",
    "    event_col=behavior_event_col\n",
    ")\n",
    "\n",
    "print(\"\\n--- Parsed Trial Summary ---\")\n",
    "with pd.option_context('display.max_rows', 200, 'display.max_columns', None):\n",
    "    display(trial_summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da237797-8d7d-4325-8031-ba39dcf5bbb7",
   "metadata": {},
   "source": [
    "## 6b. Extract flurescence traces from each ROI across trials (resumable)\n",
    "Iterate trials **by index**, load the mapped **MCOR** movie, extract traces with the cleaned ROIs, and checkpoint results per trial to `trace_cache/`:\n",
    "- `*.npz` → arrays (`F`, `dFF`, `Z`)\n",
    "- `*.json` → small per-trial metadata\n",
    "When resuming from partial runs, set `LOAD_EXISTING=True` to pre-load cached trials; set `SKIP_EXISTING=True` to skip recomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d91e55-98fe-47f1-b361-fcc6095217fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# where to cache per-trial results (npz + json)\n",
    "checkpoint_dir = Path(parent_dir) / \"trace_cache\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOAD_EXISTING  = True   # preload cached trials into memory\n",
    "SKIP_EXISTING  = True   # skip any trial already cached (by trial_name)\n",
    "\n",
    "def _trial_paths(trial_name: str):\n",
    "    npz = checkpoint_dir / f\"{trial_name}.npz\"\n",
    "    jsn = checkpoint_dir / f\"{trial_name}.json\"\n",
    "    return npz, jsn\n",
    "\n",
    "def _atomic_save_npz(path: Path, **arrays):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with tempfile.NamedTemporaryFile(mode='wb', delete=False, dir=path.parent,\n",
    "                                     prefix=path.stem + '_', suffix='.npz') as tf:\n",
    "        np.savez_compressed(tf, **arrays)\n",
    "        tmp_name = tf.name\n",
    "        tf.flush(); os.fsync(tf.fileno())\n",
    "    os.replace(tmp_name, path)\n",
    "\n",
    "def _atomic_save_json(path: Path, obj):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False, dir=path.parent,\n",
    "                                     prefix=path.stem + '_', suffix='.json') as tf:\n",
    "        json.dump(obj, tf)\n",
    "        tmp_name = tf.name\n",
    "        tf.flush(); os.fsync(tf.fileno())\n",
    "    os.replace(tmp_name, path)\n",
    "\n",
    "# --- resume: discover already-completed trials ---\n",
    "existing_trials = {p.stem for p in checkpoint_dir.glob(\"*.npz\")}\n",
    "print(f\"[resume] Found {len(existing_trials)} completed trials in {checkpoint_dir}\")\n",
    "\n",
    "all_trials_data_list = []\n",
    "missing_imaging = []\n",
    "preloaded_names = set()\n",
    "\n",
    "# (Optional) pre-load existing so memory has a complete view immediately\n",
    "\n",
    "if LOAD_EXISTING and existing_trials:\n",
    "    npz_paths = sorted(checkpoint_dir.glob(\"*.npz\"))\n",
    "    for npz_path in tqdm(npz_paths, desc=\"Loading cached trials\", unit=\"trial\", leave=False):\n",
    "        trial_name = npz_path.stem\n",
    "        json_path = checkpoint_dir / f\"{trial_name}.json\"\n",
    "\n",
    "        # ensure file handle closes promptly\n",
    "        with np.load(npz_path) as data:\n",
    "            meta = {}\n",
    "            if json_path.exists():\n",
    "                with open(json_path, \"r\") as f:\n",
    "                    meta = json.load(f)\n",
    "            all_trials_data_list.append({\n",
    "                'trial_name': trial_name,\n",
    "                'traces': {'F': data['F'], 'dFF': data['dFF'], 'Z': data['Z']},\n",
    "                'behavior_summary': meta.get('behavior_summary', {})\n",
    "            })\n",
    "        preloaded_names.add(trial_name)\n",
    "\n",
    "# --- main loop (resumable) ---\n",
    "skipped_existing = 0\n",
    "processed_new = 0\n",
    "reloaded_in_loop = 0\n",
    "\n",
    "for i in tqdm(range(len(trial_summary_df)), desc=\"Extracting fluorescence traces from ROIs defined across trials\"):\n",
    "    mcor_path = resolved_trial_to_mcor.get(i, None)\n",
    "    if mcor_path is None:\n",
    "        print(f\"Skipping behavioral trial {i}: No corresponding MCOR file found within ±{TOL_SECONDS}s\")\n",
    "        missing_imaging.append(i)\n",
    "        continue\n",
    "    if not mcor_path.exists():\n",
    "        print(f\"Skipping behavioral trial {i}: MCOR file missing on disk -> {mcor_path}\")\n",
    "        missing_imaging.append(i)\n",
    "        continue\n",
    "\n",
    "    trial_name = mcor_path.stem\n",
    "    npz_path, json_path = _trial_paths(trial_name)\n",
    "\n",
    "    # Hard skip if we already have this trial cached (by name), regardless of fs hiccups\n",
    "    if SKIP_EXISTING and (trial_name in existing_trials or npz_path.exists()):\n",
    "        if LOAD_EXISTING and trial_name not in preloaded_names:\n",
    "            # (rare) cached on disk but not preloaded yet → load once\n",
    "            data = np.load(npz_path)\n",
    "            meta = {}\n",
    "            if json_path.exists():\n",
    "                with open(json_path, \"r\") as f:\n",
    "                    meta = json.load(f)\n",
    "            all_trials_data_list.append({\n",
    "                'trial_name': trial_name,\n",
    "                'traces': {'F': data['F'], 'dFF': data['dFF'], 'Z': data['Z']},\n",
    "                'behavior_summary': meta.get('behavior_summary', {})\n",
    "            })\n",
    "            preloaded_names.add(trial_name)\n",
    "            reloaded_in_loop += 1\n",
    "        skipped_existing += 1\n",
    "        continue\n",
    "\n",
    "    # fresh processing\n",
    "    try:\n",
    "        trial_movie_raw = cm.load(str(mcor_path))\n",
    "        dims = trial_movie_raw.shape[1:]\n",
    "        if cleaned_A_list:\n",
    "            F, dFF, Z = extract_traces_for_trial(trial_movie_raw, cleaned_A_list, dims, fr)\n",
    "\n",
    "            # save first (atomic), then append\n",
    "            _atomic_save_npz(npz_path, F=F, dFF=dFF, Z=Z)\n",
    "            _atomic_save_json(json_path, {\n",
    "                \"trial_name\": trial_name,\n",
    "                \"behavior_summary\": trial_summary_df.iloc[i].to_dict()\n",
    "            })\n",
    "\n",
    "            all_trials_data_list.append({\n",
    "                'trial_name': trial_name,\n",
    "                'traces': {'F': F, 'dFF': dFF, 'Z': Z},\n",
    "                'behavior_summary': trial_summary_df.iloc[i].to_dict()\n",
    "            })\n",
    "            processed_new += 1\n",
    "            existing_trials.add(trial_name)  # mark as done for any later iterations\n",
    "    except Exception as e:\n",
    "        print(f\"[error] Trial {trial_name}: {e}\")\n",
    "    finally:\n",
    "        try:\n",
    "            del trial_movie_raw\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# --- Final safety: de-dup in memory by trial_name (keep first occurrence) ---\n",
    "unique = []\n",
    "seen = set()\n",
    "for rec in all_trials_data_list:\n",
    "    name = rec['trial_name']\n",
    "    if name in seen:\n",
    "        continue\n",
    "    seen.add(name)\n",
    "    unique.append(rec)\n",
    "all_trials_data_list = unique\n",
    "\n",
    "print(f\"\\n--- Summary ---\")\n",
    "print(f\"Preloaded existing: {len(preloaded_names)}\")\n",
    "print(f\"Skipped existing in loop: {skipped_existing} (of which {reloaded_in_loop} were loaded on-demand)\")\n",
    "print(f\"Processed new: {processed_new}\")\n",
    "print(f\"Unique trials in memory: {len(all_trials_data_list)}\")\n",
    "print(f\"Dropped {len(missing_imaging)} with no imaging match.\")\n",
    "print(f\"[resume] Per-trial checkpoints saved in: {checkpoint_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faaa798",
   "metadata": {},
   "source": [
    "## 7. Consolidated HDF5 output — layout, shapes, and dtypes\n",
    "\n",
    "The notebook writes a single HDF5 at `export_hdf5` with these groups:\n",
    "\n",
    "~~~text\n",
    "/                                  # Root\n",
    "├── roi_info/                      # Per-PIXEL ROI info (one row per pixel kept in df_pixels)\n",
    "│   ├── ROI                        # shape (P,), int64 — ROI id for each pixel\n",
    "│   ├── x                          # shape (P,), int64 — x coord (pixel)\n",
    "│   ├── y                          # shape (P,), int64 — y coord (pixel)\n",
    "│   ├── weight_norm                # shape (P,), float32/float64 — normalized spatial weight\n",
    "│   └── ...                        # any other df_pixels columns; object dtype stored as bytes (HDF5 'S')\n",
    "│\n",
    "├── trials/                        # Per-TRIAL fluorescence (one subgroup per trial)\n",
    "│   └── <trial_name>/              # e.g., 20250909_sid169_e1_00037_mcor\n",
    "│       ├── F                      # shape (R, Tᵢ), float32/float64 — raw fluorescence per ROI\n",
    "│       ├── dFF                    # shape (R, Tᵢ), float32/float64 — ΔF/F per ROI\n",
    "│       └── Z                      # shape (R, Tᵢ), float32/float64 — z-scored trace per ROI\n",
    "│\n",
    "├── AlignedBehaviorEvents/         # Behavior, retimestamped to the corrected imaging clock\n",
    "│   ├── OdorPresentations          # shape (N, 3), float64 — [odor_id, onset_rel_s, offset_rel_s]\n",
    "│   ├── OdorPresentations_posix    # shape (N, 3), float64 — [odor_id, onset_POSIX, offset_POSIX]\n",
    "│   ├── Licks                      # shape (K, 1), float64 — event times (session-relative seconds)\n",
    "│   ├── Rewards                    # shape (M, 1), float64 — event times (session-relative seconds)\n",
    "│   ├── Licks_posix                # shape (K, 1), float64 — absolute POSIX seconds\n",
    "│   ├── Rewards_posix              # shape (M, 1), float64 — absolute POSIX seconds\n",
    "│   └── TrialSummary/              # trial-wise metadata (columnar; one dataset per column)\n",
    "│       ├── trial_index            # shape (N,), int64\n",
    "│       ├── odor_id                # shape (N,), int64/float64 (your dtype)\n",
    "│       ├── outcome                # shape (N,), bytes (HDF5 'S'; values like b'Hit', b'False Choice', b'Miss')\n",
    "│       ├── trial_start_time       # shape (N,), float64 — behavior clock (seconds)\n",
    "│       ├── trial_end_time         # shape (N,), float64 — behavior clock (seconds)\n",
    "│       ├── trial_start_time_posix # shape (N,), float64 — corrected absolute POSIX seconds\n",
    "│       └── trial_start_time_corrected_s\n",
    "│                                  # shape (N,), float64 — corrected session-relative seconds\n",
    "│\n",
    "└── AbsoluteAlignment/             # How behavior↔imaging were aligned\n",
    "    ├── onsets_wall_posix_before_offset\n",
    "    │                              # shape (N,), float64 — original H5 onsets (if available)\n",
    "    ├── onsets_wall_posix_after_offset\n",
    "    │   or onsets_wall_posix       # shape (N,), float64 — the onset vector actually used\n",
    "    ├── raw_tiff_stems             # shape (M_raw,), bytes (HDF5 'S') — stems of RAW TIFFs\n",
    "    ├── raw_tiff_first_frame_posix # shape (M_raw,), float64 — first-frame POSIX per RAW TIFF\n",
    "    ├── trial_to_mcor/ (if used)   # definitive mapping used for extraction\n",
    "    │   ├── trial_index            # shape (Q,), int64 — subset of [0..N-1] that matched\n",
    "    │   └── mcor_path              # shape (Q,), bytes (HDF5 'S') — absolute paths to MCOR files\n",
    "    └── trial_to_stem/ (else)      # fallback mapping by RAW stem\n",
    "        ├── trial_index            # shape (Q,), int64\n",
    "        └── stem                   # shape (Q,), bytes (HDF5 'S')\n",
    "~~~\n",
    "\n",
    "### Attributes on /AbsoluteAlignment\n",
    "AbsoluteAlignment.attrs['h5_file']                  # str (basename if present)\n",
    "AbsoluteAlignment.attrs['h5_start_time_wall_utc_iso']# str ISO8601 (if present)\n",
    "AbsoluteAlignment.attrs['coarse_offset_applied_s']   # float (if present)\n",
    "AbsoluteAlignment.attrs['match_tolerance_s']         # float (TOL_SECONDS)\n",
    "AbsoluteAlignment.attrs['mapping_kind']              # {'trial_to_mcor','trial_to_stem','none'}\n",
    "\n",
    "### Symbol key\n",
    "- **P** = number of rows in df_pixels (per-pixel table, not number of ROIs)\n",
    "- **R** = number of ROIs kept after cleaning (length of cleaned_A_list / kept_indices_list)\n",
    "- **Tᵢ** = number of frames in trial i (can vary per trial)\n",
    "- **N** = number of parsed trials\n",
    "- **K, M** = number of lick/reward events\n",
    "- **M_raw** = number of RAW TIFF stacks found\n",
    "- **Q** = number of trials that successfully mapped to MCOR files\n",
    "\n",
    "### Time bases (very important)\n",
    "- **Session-relative seconds**: 0 is the H5 wall-clock start (h5_start_time_wall) when available; otherwise min of used onsets.\n",
    "- **POSIX seconds (absolute)**: UTC epoch seconds; use when aligning across devices/computers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154160ee-7db4-4aa4-b21f-8283662e5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Consolidate & save (uses H5-derived corrected trial timing; remaps event times) ---\n",
    "\n",
    "print(\"--- Consolidating Behavioral Data for HDF5 Output (H5-corrected) ---\")\n",
    "\n",
    "# ---- 1) Decide corrected onsets from H5 (posix) ----\n",
    "used_onsets = globals().get('h5_onsets_shifted', None)\n",
    "if used_onsets is None:\n",
    "    used_onsets = np.asarray(ground_truth_trial_onsets, dtype=float)\n",
    "else:\n",
    "    used_onsets = np.asarray(used_onsets, dtype=float)\n",
    "\n",
    "# session-zero for relative seconds (prefer H5 start wall clock if present)\n",
    "if 'h5_start_time_wall' in globals() and h5_start_time_wall is not None:\n",
    "    session_zero_posix = float(h5_start_time_wall.timestamp())\n",
    "else:\n",
    "    session_zero_posix = float(np.nanmin(used_onsets))\n",
    "\n",
    "# Make sure trial count matches\n",
    "n_trials = min(len(used_onsets), len(trial_summary_df))\n",
    "if len(used_onsets) != len(trial_summary_df):\n",
    "    print(f\"[warn] used_onsets ({len(used_onsets)}) != trial_summary_df ({len(trial_summary_df)}). \"\n",
    "          f\"Using first {n_trials} trials.\")\n",
    "\n",
    "# ---- 2) Build corrected OdorPresentations ----\n",
    "odor_ids = trial_summary_df['odor_id'].values[:n_trials]\n",
    "odor_onset_posix = used_onsets[:n_trials]\n",
    "odor_offset_posix = odor_onset_posix + float(odor_duration_s)\n",
    "\n",
    "# session-relative seconds (0 = H5 start)\n",
    "odor_onset_rel = odor_onset_posix - session_zero_posix\n",
    "odor_offset_rel = odor_offset_posix - session_zero_posix\n",
    "\n",
    "odor_matrix = np.column_stack([odor_ids, odor_onset_rel, odor_offset_rel]).astype(float)\n",
    "print(f\"  - OdorPresentations shape: {odor_matrix.shape} (H5-corrected)\")\n",
    "\n",
    "# ---- 3) Remap licks & rewards from behavior timeline -> corrected timeline ----\n",
    "# Define which strings count as licks/rewards in event log\n",
    "LICK_EVENTS   = {'Lick', 'Lick L', 'Lick R'}\n",
    "REWARD_EVENTS = {'Reward', 'Reward L', 'Reward R'}\n",
    "\n",
    "# Trial windows on the behavior timeline\n",
    "trial_starts_beh = trial_summary_df['trial_start_time'].values[:n_trials].astype(float)\n",
    "if 'trial_end_time' in trial_summary_df.columns:\n",
    "    trial_ends_beh = trial_summary_df['trial_end_time'].values[:n_trials].astype(float)\n",
    "else:\n",
    "    # Next start or max time if end col missing\n",
    "    trial_ends_beh = np.r_[trial_starts_beh[1:], float(master_events_df[behavior_time_col].max())]\n",
    "\n",
    "# Helper: map each event time -> trial index (behavior timeline), then shift\n",
    "def _events_to_corrected_times(df, names):\n",
    "    ev = df[df[behavior_event_col].isin(names)][[behavior_time_col, behavior_event_col]].copy()\n",
    "    if ev.empty:\n",
    "        return np.empty((0, 1)), np.empty((0, 1))  # rel, posix\n",
    "\n",
    "    t = ev[behavior_time_col].values.astype(float)\n",
    "    # candidate trial index by start times\n",
    "    idx = np.searchsorted(trial_starts_beh, t, side='right') - 1\n",
    "    valid = (idx >= 0) & (idx < n_trials) & (t < trial_ends_beh[idx])\n",
    "    ev = ev.loc[valid]\n",
    "    if ev.empty:\n",
    "        return np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "    t = ev[behavior_time_col].values.astype(float)\n",
    "    idx = np.searchsorted(trial_starts_beh, t, side='right') - 1\n",
    "\n",
    "    # per-event shift = (H5 onset posix) - (behavior trial start secs)\n",
    "    shift = odor_onset_posix[idx] - trial_starts_beh[idx]\n",
    "    abs_posix = t + shift                  # absolute POSIX seconds\n",
    "    rel_secs  = abs_posix - session_zero_posix  # session-relative seconds\n",
    "\n",
    "    return rel_secs.reshape(-1, 1), abs_posix.reshape(-1, 1)\n",
    "\n",
    "licks_rel,   licks_posix   = _events_to_corrected_times(master_events_df, LICK_EVENTS)\n",
    "rewards_rel, rewards_posix = _events_to_corrected_times(master_events_df, REWARD_EVENTS)\n",
    "\n",
    "# ---- 4) Augment trial_summary_df with corrected start times for saving ----\n",
    "trial_summary_df['trial_start_time_posix']       = np.nan\n",
    "trial_summary_df['trial_start_time_corrected_s'] = np.nan\n",
    "trial_summary_df.loc[:n_trials-1, 'trial_start_time_posix']       = odor_onset_posix\n",
    "trial_summary_df.loc[:n_trials-1, 'trial_start_time_corrected_s'] = odor_onset_rel\n",
    "\n",
    "# ---- 5) Persist everything ----\n",
    "with h5py.File(export_hdf5, 'w') as f:\n",
    "    # ROI spatial info\n",
    "    roi_info_group = f.create_group('roi_info')\n",
    "    for col in df_pixels.columns:\n",
    "        data = df_pixels[col].values\n",
    "        if df_pixels[col].dtype == 'object':\n",
    "            data = data.astype(str).astype(np.string_)\n",
    "        roi_info_group.create_dataset(col, data=data)\n",
    "\n",
    "    # Per-trial fluorescence traces\n",
    "    trials_group = f.create_group('trials')\n",
    "    for trial_data in all_trials_data_list:\n",
    "        trial_group = trials_group.create_group(trial_data['trial_name'])\n",
    "        for trace_type, trace_data in trial_data['traces'].items():\n",
    "            trial_group.create_dataset(trace_type, data=trace_data)\n",
    "\n",
    "    # Aligned behavior (session-wide, H5-corrected)\n",
    "    behavior_group = f.create_group('AlignedBehaviorEvents')\n",
    "    behavior_group.create_dataset('OdorPresentations', data=odor_matrix)                 # [odor_id, onset_rel_s, offset_rel_s]\n",
    "    behavior_group.create_dataset('OdorPresentations_posix', data=np.column_stack([odor_ids, odor_onset_posix, odor_offset_posix]))\n",
    "    behavior_group.create_dataset('Licks',   data=licks_rel)\n",
    "    behavior_group.create_dataset('Rewards', data=rewards_rel)\n",
    "    behavior_group.create_dataset('Licks_posix',   data=licks_posix)\n",
    "    behavior_group.create_dataset('Rewards_posix', data=rewards_posix)\n",
    "    behavior_group.attrs['time_basis'] = 'session-relative seconds (0 = h5_start_time_wall), plus *_posix in absolute seconds'\n",
    "\n",
    "    # Trial summary table (+ corrected columns)\n",
    "    trial_summary_group = behavior_group.create_group('TrialSummary')\n",
    "    for col in trial_summary_df.columns:\n",
    "        data = trial_summary_df[col].values\n",
    "        if data.dtype == 'object':\n",
    "            data = data.astype(str).astype(np.string_)\n",
    "        trial_summary_group.create_dataset(col, data=data)\n",
    "\n",
    "    # Absolute alignment details (keep existing attrs/datasets)\n",
    "    align_group = f.create_group('AbsoluteAlignment')\n",
    "    if 'h5_path' in globals():\n",
    "        align_group.attrs['h5_file'] = str(h5_path.name)\n",
    "    if 'h5_start_time_wall' in globals() and h5_start_time_wall is not None:\n",
    "        align_group.attrs['h5_start_time_wall_utc_iso'] = h5_start_time_wall.isoformat()\n",
    "\n",
    "    if 'ground_truth_trial_onsets' in globals():\n",
    "        align_group.create_dataset('onsets_wall_posix_before_offset', data=np.asarray(ground_truth_trial_onsets, dtype=float))\n",
    "    # Save whichever set was actually used\n",
    "    label_used = 'onsets_wall_posix_after_offset' if 'h5_onsets_shifted' in globals() and h5_onsets_shifted is not None else 'onsets_wall_posix'\n",
    "    align_group.create_dataset(label_used, data=np.asarray(used_onsets, dtype=float))\n",
    "\n",
    "    # RAW TIFF timing\n",
    "    raw_tiff_stems = np.array([s for (s, _) in raw_onsets_records], dtype=object).astype('S')\n",
    "    raw_tiff_first_frame_posix = np.array([t for (_, t) in raw_onsets_records], dtype=float)\n",
    "    align_group.create_dataset('raw_tiff_stems', data=raw_tiff_stems)\n",
    "    align_group.create_dataset('raw_tiff_first_frame_posix', data=raw_tiff_first_frame_posix)\n",
    "\n",
    "    # Mapping info\n",
    "    mapping_kind = None\n",
    "    if 'resolved_trial_to_mcor' in globals() and resolved_trial_to_mcor:\n",
    "        mapping_kind = 'trial_to_mcor'\n",
    "    elif 'trial_to_stem' in globals() and trial_to_stem:\n",
    "        mapping_kind = 'trial_to_stem'\n",
    "    else:\n",
    "        mapping_kind = 'none'\n",
    "    align_group.attrs['mapping_kind'] = mapping_kind\n",
    "    if mapping_kind == 'trial_to_mcor':\n",
    "        map_grp = align_group.create_group('trial_to_mcor')\n",
    "        map_grp.create_dataset('trial_index', data=np.array(list(resolved_trial_to_mcor.keys()), dtype=int))\n",
    "        map_grp.create_dataset('mcor_path', data=np.array([str(p) for p in resolved_trial_to_mcor.values()], dtype=object).astype('S'))\n",
    "        map_grp.attrs['resolver'] = 'trailing_number_only'\n",
    "    elif mapping_kind == 'trial_to_stem':\n",
    "        map_grp = align_group.create_group('trial_to_stem')\n",
    "        map_grp.create_dataset('trial_index', data=np.array(list(trial_to_stem.keys()), dtype=int))\n",
    "        map_grp.create_dataset('stem', data=np.array(list(trial_to_stem.values()), dtype=object).astype('S'))\n",
    "\n",
    "    # Alignment params\n",
    "    if 'offset_s' in globals():\n",
    "        align_group.attrs['coarse_offset_applied_s'] = float(offset_s)\n",
    "    if 'TOL_SECONDS' in globals():\n",
    "        align_group.attrs['match_tolerance_s'] = float(TOL_SECONDS)\n",
    "\n",
    "print(f\"\\n--- Saved all outputs to: {export_hdf5} ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d7cc8",
   "metadata": {},
   "source": [
    "## 8) Interactive ROI Explorer\n",
    "For each selected ROI, show its footprint on the max-projection and a column of subplots (one per odor). \n",
    "\n",
    "A legend shows the number of each trial outcome per odor.\n",
    "\n",
    "Traces are **color-coded by outcome** and overlaid with the per-outcome mean. \n",
    "\n",
    "The odor window is highlighted for quick inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd541ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_pixels.empty or not all_trials_data_list:\n",
    "    print(\"ROI explorer skipped (df_pixels/all_trials_data_list missing).\")\n",
    "else:\n",
    "    kept_roi_ids = sorted(df_pixels['ROI'].unique())\n",
    "    max_proj_img = max_proj\n",
    "    dims = max_proj_img.shape\n",
    "    kept_indices_list = clean_results['kept_indices']\n",
    "\n",
    "    # Odors present in this session\n",
    "    unique_odor_ids = sorted(list(set(t['behavior_summary']['odor_id'] for t in all_trials_data_list)))\n",
    "\n",
    "    # Colors by outcome (per your request)\n",
    "    outcome_color_map = {\n",
    "        'Hit':          (0.000, 0.447, 0.698),  # #0072B2\n",
    "        'False Choice': (0.800, 0.475, 0.741),  # #CC79A7\n",
    "        'Miss':         (0.941, 0.894, 0.559),  # #F0E442\n",
    "    }\n",
    "    outcome_order = ['Miss', 'False Choice', 'Hit']\n",
    "\n",
    "    roi_slider = widgets.SelectionSlider(options=kept_roi_ids, value=kept_roi_ids[0], description='Select ROI ID:',\n",
    "                                         continuous_update=False, orientation='horizontal', readout=True,\n",
    "                                         style={'description_width': 'initial'}, layout={'width': '80%'})\n",
    "    prev_button = widgets.Button(description=\"< Previous\", layout={'width': '100px'})\n",
    "    next_button = widgets.Button(description=\"Next >\", layout={'width': '100px'})\n",
    "    plot_output = widgets.Output()\n",
    "\n",
    "    def plot_for_roi_id(roi_id):\n",
    "        with plot_output:\n",
    "            plot_output.clear_output(wait=True)\n",
    "\n",
    "            # ---- figure: left footprint + right column of odor subplots ----\n",
    "            fig_h = max(6, 3.2 * max(1, len(unique_odor_ids)))  # scale height with number of odors\n",
    "            fig = plt.figure(figsize=(14, fig_h))\n",
    "            gs = fig.add_gridspec(1, 2, width_ratios=[2, 3])\n",
    "            ax_footprint = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "            # Left: footprint (unchanged)\n",
    "            ax_footprint.set_title(f\"ROI {roi_id} Footprint\")\n",
    "            ax_footprint.imshow(max_proj_img, cmap='gray')\n",
    "            roi_footprint_img = np.zeros(dims)\n",
    "            sub_df = df_pixels[df_pixels['ROI'] == roi_id]\n",
    "            if not sub_df.empty:\n",
    "                roi_footprint_img[sub_df['y'].astype(int), sub_df['x'].astype(int)] = sub_df['weight_norm']\n",
    "                ax_footprint.contour(roi_footprint_img,\n",
    "                                     levels=[0.2 * roi_footprint_img.max()],\n",
    "                                     colors='cyan', linewidths=2)\n",
    "            ax_footprint.axis('off')\n",
    "\n",
    "            # Right: one subplot per odor, traces colored by OUTCOME\n",
    "            try:\n",
    "                roi_index = kept_indices_list.index(roi_id)\n",
    "                n_odors = len(unique_odor_ids)\n",
    "                if n_odors == 0:\n",
    "                    ax_placeholder = fig.add_subplot(gs[0, 1])\n",
    "                    ax_placeholder.text(0.5, 0.5, 'No odors found', ha='center', va='center')\n",
    "                    ax_placeholder.axis('off')\n",
    "                else:\n",
    "                    # ---------- precompute global y-lims across all odors ----------\n",
    "                    per_odor = {}\n",
    "                    global_ymin, global_ymax = np.inf, -np.inf\n",
    "                    for oid in unique_odor_ids:\n",
    "                        traces_by_outcome = {k: [] for k in outcome_order}\n",
    "                        for trial_info in all_trials_data_list:\n",
    "                            if trial_info['behavior_summary'].get('odor_id') != oid:\n",
    "                                continue\n",
    "                            outcome = trial_info['behavior_summary'].get('outcome', 'Unknown')\n",
    "                            if outcome not in traces_by_outcome:\n",
    "                                continue\n",
    "                            dff_trace = trial_info['traces']['dFF'][roi_index, :]\n",
    "                            traces_by_outcome[outcome].append(dff_trace)\n",
    "\n",
    "                        all_lists = [traces_by_outcome[o] for o in outcome_order if len(traces_by_outcome[o]) > 0]\n",
    "                        if all_lists:\n",
    "                            min_len = min(min(len(arr) for arr in lst) for lst in all_lists)\n",
    "                        else:\n",
    "                            min_len = None\n",
    "\n",
    "                        per_odor[oid] = (traces_by_outcome, min_len)\n",
    "\n",
    "                        if min_len is not None and min_len > 0:\n",
    "                            for lst in all_lists:\n",
    "                                for arr in lst:\n",
    "                                    seg = arr[:min_len]\n",
    "                                    if seg.size:\n",
    "                                        vmin = float(seg.min()); vmax = float(seg.max())\n",
    "                                        if vmin < global_ymin: global_ymin = vmin\n",
    "                                        if vmax > global_ymax: global_ymax = vmax\n",
    "\n",
    "                    if np.isfinite(global_ymin) and np.isfinite(global_ymax):\n",
    "                        span = global_ymax - global_ymin\n",
    "                        pad = 0.05 * span if span != 0 else (0.1 if global_ymax == 0 else abs(global_ymax) * 0.1)\n",
    "                        ylo, yhi = global_ymin - pad, global_ymax + pad\n",
    "                    else:\n",
    "                        ylo = yhi = None\n",
    "                    # -------------------------------------------------------------------\n",
    "\n",
    "                    # Column layout: nrows = number of odors, 1 column\n",
    "                    subgs = gs[0, 1].subgridspec(n_odors, 1, hspace=0.35)\n",
    "                    axes = [fig.add_subplot(subgs[j, 0]) for j in range(n_odors)]\n",
    "\n",
    "                    for ax, oid in zip(axes, unique_odor_ids):\n",
    "                        ax.set_title(f\"ROI {roi_id} • Odor {int(oid)} (by outcome)\")\n",
    "\n",
    "                        traces_by_outcome, min_len = per_odor[oid]\n",
    "\n",
    "                        # Plot individual transparent traces + thick mean per outcome\n",
    "                        if min_len is not None and min_len > 0:\n",
    "                            time_s = np.arange(min_len) / fr\n",
    "                            for outcome in outcome_order:\n",
    "                                color = outcome_color_map[outcome]\n",
    "                                arrs = traces_by_outcome[outcome]\n",
    "                                if not arrs:\n",
    "                                    continue\n",
    "                                for arr in arrs:  # transparent individuals\n",
    "                                    ax.plot(time_s, arr[:min_len], color=color, lw=0.8, alpha=0.2)\n",
    "                                mean_trace = np.mean([arr[:min_len] for arr in arrs], axis=0)  # mean on top\n",
    "                                ax.plot(time_s, mean_trace, color=color, lw=1.5, alpha=1.0, zorder=5)\n",
    "\n",
    "                        # Cosmetics\n",
    "                        ax.axvspan(5, 6, color='gray', alpha=0.15)\n",
    "                        ax.set_xlabel('Time (s)'); ax.set_ylabel('dF/F')\n",
    "                        ax.spines[['top','right']].set_visible(False)\n",
    "                        ax.axhline(0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "                        # Apply shared y-lims\n",
    "                        if ylo is not None:\n",
    "                            ax.set_ylim(ylo, yhi)\n",
    "\n",
    "                        # ---------- NEW: per-odor legend with counts ----------\n",
    "                        n_hit = len(traces_by_outcome.get('Hit', []))\n",
    "                        n_fc  = len(traces_by_outcome.get('False Choice', []))\n",
    "                        n_m   = len(traces_by_outcome.get('Miss', []))\n",
    "                        legend_elems = [\n",
    "                            Line2D([0], [0], color=outcome_color_map['Hit'],          lw=1.5, label=f'Hit: {n_hit}'),\n",
    "                            Line2D([0], [0], color=outcome_color_map['False Choice'], lw=1.5, label=f'FC: {n_fc}'),\n",
    "                            Line2D([0], [0], color=outcome_color_map['Miss'],         lw=1.5, label=f'M: {n_m}'),\n",
    "                        ]\n",
    "                        ax.legend(handles=legend_elems, loc='upper left', frameon=False, fontsize=9)\n",
    "                        # ----------------------------------------------------\n",
    "\n",
    "                    # Shared legend (kept)\n",
    "                    outcome_legend = [\n",
    "                        Line2D([0], [0], color=outcome_color_map['Hit'],          lw=1.5, label='Hit'),\n",
    "                        Line2D([0], [0], color=outcome_color_map['False Choice'], lw=1.5, label='False Choice'),\n",
    "                        Line2D([0], [0], color=outcome_color_map['Miss'],         lw=1.5, label='Miss'),\n",
    "                    ]\n",
    "                    fig.legend(handles=outcome_legend, loc='upper right', frameon=False)\n",
    "\n",
    "            except (ValueError, IndexError):\n",
    "                ax_err = fig.add_subplot(gs[0, 1])\n",
    "                ax_err.text(0.5, 0.5, 'Index error', ha='center', va='center')\n",
    "                ax_err.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def on_button_click(button):\n",
    "        current_index = kept_roi_ids.index(roi_slider.value)\n",
    "        new_index = current_index - 1 if button.description.startswith('<') else current_index + 1\n",
    "        new_index = max(0, min(len(kept_roi_ids) - 1, new_index))\n",
    "        roi_slider.value = kept_roi_ids[new_index]\n",
    "\n",
    "    def on_slider_change(change):\n",
    "        plot_for_roi_id(change.new)\n",
    "\n",
    "    prev_button.on_click(on_button_click); next_button.on_click(on_button_click)\n",
    "    roi_slider.observe(on_slider_change, names='value')\n",
    "    ui_controls = HBox([prev_button, roi_slider, next_button])\n",
    "    full_ui = VBox([ui_controls, plot_output])\n",
    "    plot_for_roi_id(roi_slider.value)\n",
    "    display(full_ui)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e9286",
   "metadata": {},
   "source": [
    "## Cleanup: CaImAn temporary artifacts\n",
    "\n",
    "This cell removes large, intermediate files created during processing:\n",
    "\n",
    "- **Memmaps** (`*.mmap`) in the **current working directory** (e.g., `*.mmap`)\n",
    "- **Memmaps** (`*.mmap`) in `~/caiman_data/temp`\n",
    "- **Sampling movie(s)** saved in `~/caiman_data/temp` whose names contain `\"sampling\"`  \n",
    "  (matches: `*sampling*.tif`, `*sampling*.tiff`, `*sampling*.avi`, `*sampling*.mp4`)\n",
    "\n",
    "A summary of deleted files and total freed disk space is printed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1413d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CaImAn temp cleanup ---\n",
      "No temporary files found.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "def human_bytes(n, suffix='B'):\n",
    "    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n",
    "        if abs(n) < 1024.0:\n",
    "            return f\"{n:3.1f}{unit}{suffix}\"\n",
    "        n /= 1024.0\n",
    "    return f\"{n:.1f}Y{suffix}\"\n",
    "\n",
    "# Paths & patterns\n",
    "caiman_temp_dir = Path.home() / 'caiman_data' / 'temp'\n",
    "local_mmap_pattern = '*.mmap'  # common CaImAn memmap prefix in CWD\n",
    "sampling_patterns = ['*sampling*.tif', '*sampling*.tiff', '*sampling*.avi', '*sampling*.mp4']\n",
    "\n",
    "# Collect targets\n",
    "targets = set()\n",
    "\n",
    "# 1) Memmaps in session working directory (e.g., Yr_* or caiman_*.mmap saved locally)\n",
    "targets.update(Path('.').glob(local_mmap_pattern))\n",
    "\n",
    "# 2) Memmaps under ~/caiman_data/temp\n",
    "if caiman_temp_dir.is_dir():\n",
    "    targets.update(caiman_temp_dir.glob('*.mmap'))\n",
    "    # 3) Sampling movie(s) placed in the temp dir\n",
    "    for pat in sampling_patterns:\n",
    "        targets.update(caiman_temp_dir.glob(pat))\n",
    "\n",
    "# Deduplicate, resolve, and filter existing files\n",
    "targets = {p.resolve() for p in targets if p.exists()}\n",
    "\n",
    "print(\"--- CaImAn temp cleanup ---\")\n",
    "if not targets:\n",
    "    print(\"No temporary files found.\")\n",
    "else:\n",
    "    total_bytes = 0\n",
    "    deleted = 0\n",
    "    for f_path in sorted(targets):\n",
    "        try:\n",
    "            size = f_path.stat().st_size\n",
    "        except FileNotFoundError:\n",
    "            size = 0\n",
    "        try:\n",
    "            f_path.unlink()\n",
    "            deleted += 1\n",
    "            total_bytes += size\n",
    "            print(f\"  - Deleted: {f_path} ({human_bytes(size)})\")\n",
    "        except OSError as e:\n",
    "            print(f\"  - ERROR deleting {f_path}: {e}\")\n",
    "\n",
    "    print(f\"Deleted {deleted} file(s); freed ~{human_bytes(total_bytes)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270f9cd-fd56-4d9f-977f-ef94dccc9497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
